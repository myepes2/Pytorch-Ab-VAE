{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd08b5eae59158eb5df8dccd4d9a2b161bdb012614f323f302a97c6f24cb2720dd4",
   "display_name": "Python 3.7.10 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.7.1\nFalse\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEResBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=5) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channels)\n",
    "\n",
    "        self.bn2 = torch.nn.BatchNorm2d(in_channels * 2)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels * 2,\n",
    "            kernel_size=5,\n",
    "            stride=2,\n",
    "            padding=2\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels * 2,\n",
    "            in_channels * 2,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "        )\n",
    "\n",
    "        self.conv_skip = torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels * 2,\n",
    "            kernel_size=5,\n",
    "            stride=2,\n",
    "            padding=2\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"input: \", x.shape)\n",
    "        out = self.conv1(self.activation(self.bn1(x)))\n",
    "        print(\"conv1: \", out.shape)\n",
    "        out = self.conv2(self.activation(self.bn2(out)))\n",
    "        print(\"conv2: \", out.shape)\n",
    "        out += self.conv_skip(x)\n",
    "        print(\"+skip: \", out.shape)\n",
    "        #out = self.pool(out)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reverse_VAEResBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=5) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channels)\n",
    "\n",
    "        self.bn2 = torch.nn.BatchNorm2d(in_channels // 2)\n",
    "\n",
    "        self.deconv1 = torch.nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            in_channels // 2,\n",
    "            kernel_size=5,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            output_padding = 1\n",
    "        )\n",
    "\n",
    "\n",
    "        self.deconv2 = torch.nn.ConvTranspose2d(\n",
    "            in_channels // 2,\n",
    "            in_channels // 2,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "        )\n",
    "\n",
    "        self.deconv_skip = torch.nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            in_channels // 2,\n",
    "            kernel_size=5,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            output_padding = 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"recode: \", x.shape)\n",
    "        out = self.deconv1(self.activation(self.bn1(x)))\n",
    "        print(\"deconv1: \", out.shape)\n",
    "        out = self.deconv2(self.activation(self.bn2(out)))\n",
    "        print(\"deconv2: \", out.shape)\n",
    "        out += self.deconv_skip(x)\n",
    "        print(\"+skip: \", out.shape)\n",
    "        #out = self.pool(out)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "boop = nn.ConvTranspose2d(16, 8, kernel_size= 5, stride = 2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Flat size:  4096\ninput:  torch.Size([1, 4, 32, 32])\nconv1:  torch.Size([1, 8, 16, 16])\nconv2:  torch.Size([1, 8, 16, 16])\n+skip:  torch.Size([1, 8, 16, 16])\n---\nPassing to decoder\n---\nrecode:  torch.Size([1, 8, 16, 16])\ndeconv1:  torch.Size([1, 4, 32, 32])\ndeconv2:  torch.Size([1, 4, 32, 32])\n+skip:  torch.Size([1, 4, 32, 32])\nFlat size:  4096\n---\nDown by factor of:  1.0\n"
     ]
    }
   ],
   "source": [
    "img_shape = (4, 32, 32)\n",
    "\n",
    "tst_img = torch.randn(1, *img_shape)\n",
    "\n",
    "print(\"Flat size: \", torch.numel(tst_img))\n",
    "\n",
    "tst_res = VAEResBlock(img_shape[0])\n",
    "\n",
    "new = tst_res(tst_img)\n",
    "\n",
    "tst_res2 = Reverse_VAEResBlock(new.shape[1])\n",
    "\n",
    "#tst_res2 = nn.ConvTranspose2d(8, 4, kernel_size=5, stride = 2, padding = 2, output_padding = 1)\n",
    "\n",
    "#tst_res2 = nn.ConvTranspose2d(8, 4, kernel_size=3, stride = 2)\n",
    "\n",
    "#tst_res2 = VAEResBlock(new.shape[1])\n",
    "\n",
    "print(\"---\\nPassing to decoder\\n---\")\n",
    "\n",
    "new = tst_res2(new)\n",
    "#print(\"deconv: \", new.shape)\n",
    "\n",
    "#tst_res3 = nn.ConvTranspose2d(4, 4, kernel_size=5, stride = 1, padding = 2)\n",
    "\n",
    "#new = tst_res3(new)\n",
    "#print(\"deconv2: \", new.shape)\n",
    "\n",
    "print(\"Flat size: \", torch.numel(new))\n",
    "print(\"---\")\n",
    "print(\"Down by factor of: \", torch.numel(tst_img)/torch.numel(new))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "np.log2(4096/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2048.0"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "(256*256)/(2**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "1*256*256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "'/home/myepes/Pytorch-Ab-VAE/img_output/tensorboard/default/version_13/checkpoints/epoch=2-step=1172.ckpt'.split('=')[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 2 3 4 5 6 7 8 9]\n[0 0 1 1 2 2 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(1,10)\n",
    "y = (x - 1) // 2\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "\n",
    "(lambda x : (x % 2))(x)"
   ]
  },
  {
   "source": [
    "x = 4\n",
    "x/=2\n",
    "x"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 162,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "metadata": {},
     "execution_count": 162
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lit_Ab_VAE import Ab_VAE\n",
    "\n",
    "model = Ab_VAE((3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Ab_VAE(\n",
       "  (encoder): Ab_Encoder(\n",
       "    (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer): Sequential(\n",
       "      (0): F_ResBlock(\n",
       "        (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(4, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "        (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (conv_skip): Conv2d(4, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      )\n",
       "      (1): F_ResBlock(\n",
       "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (conv_skip): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Ab_Decoder(\n",
       "    (linear): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (unflatten): Unflatten(dim=1, unflattened_size=(16, 8, 8))\n",
       "    (layer): Sequential(\n",
       "      (0): R_ResBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (deconv1): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "        (deconv2): ConvTranspose2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (deconv_skip): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "      )\n",
       "      (1): R_ResBlock(\n",
       "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (deconv1): ConvTranspose2d(8, 4, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "        (deconv2): ConvTranspose2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (deconv_skip): ConvTranspose2d(8, 4, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (conv): Conv2d(4, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc_var): Linear(in_features=1024, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#from Lit_Ab_VAE import Ab_VAE\n",
    "\n",
    "batch_size=128\n",
    "dataset_path = './img_datasets'\n",
    "output_path = './img_output'\n",
    "#checkpoint_file = './img_output/tensorboard/default/version_0/checkpoints/tst.ckpt'\n",
    "checkpoint_file = '/home/myepes/Pytorch-Ab-VAE/img_output/tensorboard/default/version_6/checkpoints/epoch=4-step=1954.ckpt'\n",
    "\n",
    "model.load_from_checkpoint(checkpoint_path=checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-afc9f3753309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#x = x.to(DEVICE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(x_hat.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pytorch-Ab-VAE/Lit_Ab_VAE.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf\"train_{k}\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pytorch-Ab-VAE/Lit_Ab_VAE.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, dataset, random_split\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "kwargs = {'num_workers': 16, 'pin_memory': True} \n",
    "\n",
    "train_dataset = CIFAR10(dataset_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = CIFAR10(dataset_path, transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=128, shuffle=False,  **kwargs)\n",
    "\n",
    "def draw_sample_image(x, postfix):\n",
    "  \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Visualization of {}\".format(postfix))\n",
    "    plt.imshow(np.transpose(make_grid(x, padding=2, normalize=True), (1, 2, 0)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(test_loader):\n",
    "\n",
    "        #x = x.to(DEVICE)\n",
    "        x_hat = model(x)\n",
    "        model.validation_step(x, batch_idx)\n",
    "        #print(x.shape)\n",
    "        #print(x_hat.shape)\n",
    "        draw_sample_image(x[:batch_size//2], \"Ground-truth images\")\n",
    "        draw_sample_image(x_hat[:batch_size//2], \"Reconstructed images\")\n",
    " \n",
    "        #print(\"perplexity: \", perplexity.item(),\"commit_loss: \", commitment_loss.item(), \"  codebook loss: \", codebook_loss.item())\n",
    "        if batch_idx == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload = False\n",
    "if reload:\n",
    "    updated_checkpoint_files = list(glob(os.path.join(checkpoint_path, \"*.ckpt\")))\n",
    "    if len(updated_checkpoint_files) > 0:\n",
    "        new_checkpoint_file = max(checkpoint_files, key=os.path.getctime)\n",
    "        if checkpoint_file == new_checkpoint_file:\n",
    "            print('No new checkpoint')\n",
    "        reloaded_model = Ab_VAE.load_from_checkpoint(checkpoint_path=checkpoint_file)\n",
    "        reloaded_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, y) in enumerate(test_loader):\n",
    "                x_hat = reloaded_model(x)\n",
    "                loss, logs = reloaded_model.step((x,y), batch_idx)\n",
    "                print(loss)\n",
    "                print(*logs)\n",
    "                draw_sample_image(x[:batch_size//2], \"Ground-truth images (R)\", output_path )\n",
    "                draw_sample_image(x_hat[:batch_size//2], \"Reconstructed images(R)\", output_path )\n",
    "            \n",
    "                if batch_idx == 0:\n",
    "                    break\n",
    "\n",
    "else:\n",
    "    print('Not Reloading')"
   ]
  }
 ]
}