{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd08b5eae59158eb5df8dccd4d9a2b161bdb012614f323f302a97c6f24cb2720dd4",
   "display_name": "Python 3.7.10 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.1\nFalse\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEResBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=5) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channels)\n",
    "\n",
    "        self.bn2 = torch.nn.BatchNorm2d(in_channels * 2)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels * 2,\n",
    "            kernel_size=5,\n",
    "            stride=2,\n",
    "            padding=2\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels * 2,\n",
    "            in_channels * 2,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "        )\n",
    "\n",
    "        self.conv_skip = torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels * 2,\n",
    "            kernel_size=5,\n",
    "            stride=2,\n",
    "            padding=2\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"input: \", x.shape)\n",
    "        out = self.conv1(self.activation(self.bn1(x)))\n",
    "        print(\"conv1: \", out.shape)\n",
    "        out = self.conv2(self.activation(self.bn2(out)))\n",
    "        print(\"conv2: \", out.shape)\n",
    "        out += self.conv_skip(x)\n",
    "        print(\"+skip: \", out.shape)\n",
    "        #out = self.pool(out)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reverse_VAEResBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=5) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channels)\n",
    "\n",
    "        self.bn2 = torch.nn.BatchNorm2d(in_channels // 2)\n",
    "\n",
    "        self.deconv1 = torch.nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            in_channels // 2,\n",
    "            kernel_size=5,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            output_padding = 1\n",
    "        )\n",
    "\n",
    "\n",
    "        self.deconv2 = torch.nn.ConvTranspose2d(\n",
    "            in_channels // 2,\n",
    "            in_channels // 2,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "        )\n",
    "\n",
    "        self.deconv_skip = torch.nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            in_channels // 2,\n",
    "            kernel_size=5,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            output_padding = 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"recode: \", x.shape)\n",
    "        out = self.deconv1(self.activation(self.bn1(x)))\n",
    "        print(\"deconv1: \", out.shape)\n",
    "        out = self.deconv2(self.activation(self.bn2(out)))\n",
    "        print(\"deconv2: \", out.shape)\n",
    "        out += self.deconv_skip(x)\n",
    "        print(\"+skip: \", out.shape)\n",
    "        #out = self.pool(out)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "boop = nn.ConvTranspose2d(16, 8, kernel_size= 5, stride = 2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Flat size:  4096\ninput:  torch.Size([1, 4, 32, 32])\nconv1:  torch.Size([1, 8, 16, 16])\nconv2:  torch.Size([1, 8, 16, 16])\n+skip:  torch.Size([1, 8, 16, 16])\n---\nPassing to decoder\n---\nrecode:  torch.Size([1, 8, 16, 16])\ndeconv1:  torch.Size([1, 4, 32, 32])\ndeconv2:  torch.Size([1, 4, 32, 32])\n+skip:  torch.Size([1, 4, 32, 32])\nFlat size:  4096\n---\nDown by factor of:  1.0\n"
     ]
    }
   ],
   "source": [
    "img_shape = (4, 32, 32)\n",
    "\n",
    "tst_img = torch.randn(1, *img_shape)\n",
    "\n",
    "print(\"Flat size: \", torch.numel(tst_img))\n",
    "\n",
    "tst_res = VAEResBlock(img_shape[0])\n",
    "\n",
    "new = tst_res(tst_img)\n",
    "\n",
    "tst_res2 = Reverse_VAEResBlock(new.shape[1])\n",
    "\n",
    "#tst_res2 = nn.ConvTranspose2d(8, 4, kernel_size=5, stride = 2, padding = 2, output_padding = 1)\n",
    "\n",
    "#tst_res2 = nn.ConvTranspose2d(8, 4, kernel_size=3, stride = 2)\n",
    "\n",
    "#tst_res2 = VAEResBlock(new.shape[1])\n",
    "\n",
    "print(\"---\\nPassing to decoder\\n---\")\n",
    "\n",
    "new = tst_res2(new)\n",
    "#print(\"deconv: \", new.shape)\n",
    "\n",
    "#tst_res3 = nn.ConvTranspose2d(4, 4, kernel_size=5, stride = 1, padding = 2)\n",
    "\n",
    "#new = tst_res3(new)\n",
    "#print(\"deconv2: \", new.shape)\n",
    "\n",
    "print(\"Flat size: \", torch.numel(new))\n",
    "print(\"---\")\n",
    "print(\"Down by factor of: \", torch.numel(tst_img)/torch.numel(new))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "np.log2(4096/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2048.0"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "(256*256)/(2**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "1*256*256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 2 3 4 5 6 7 8 9]\n[0 0 1 1 2 2 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(1,10)\n",
    "y = (x - 1) // 2\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "\n",
    "(lambda x : (x % 2))(x)"
   ]
  },
  {
   "source": [
    "x = 4\n",
    "x/=2\n",
    "x"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 162,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "metadata": {},
     "execution_count": 162
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lit_Ab_VAE import Ab_VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'res_channel' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-868a8380f39a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mAb_VAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Pytorch-Ab-VAE/Lit_Ab_VAE.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, latent_dim, num_blocks, lr)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_c\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_h\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAb_Encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAb_Decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pytorch-Ab-VAE/Lit_Ab_VAE.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_channels, res_channels, num_blocks)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res_channel' is not defined"
     ]
    }
   ],
   "source": [
    "img_shape = (3,32,32)\n",
    "model = Ab_VAE(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}