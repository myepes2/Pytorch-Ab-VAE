{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "dataset_path = '~/datasets'\n",
    "\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "img_size = (32, 32) # (width, height)\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 128\n",
    "n_embeddings= 768\n",
    "output_dim = 3\n",
    "\n",
    "lr = 2e-4\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "print_step = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Step 1. Load (or download) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_dataset = CIFAR10(dataset_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = CIFAR10(dataset_path, transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define our model: Vector Quantized Variational AutoEncoder (VQ-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=(4, 4, 3, 1), stride=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        kernel_1, kernel_2, kernel_3, kernel_4 = kernel_size\n",
    "        \n",
    "        self.strided_conv_1 = nn.Conv2d(input_dim, hidden_dim, kernel_1, stride, padding=1)\n",
    "        self.strided_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, stride, padding=1)\n",
    "        \n",
    "        self.residual_conv_1 = nn.Conv2d(hidden_dim, hidden_dim, kernel_3, padding=1)\n",
    "        self.residual_conv_2 = nn.Conv2d(hidden_dim, output_dim, kernel_4, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.strided_conv_1(x)\n",
    "        x = self.strided_conv_2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        y = self.residual_conv_1(x)\n",
    "        y = y+x\n",
    "        \n",
    "        x = F.relu(y)\n",
    "        y = self.residual_conv_2(x)\n",
    "        y = y+x\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQEmbeddingEMA(nn.Module):\n",
    "    def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25, decay=0.999, epsilon=1e-5):\n",
    "        super(VQEmbeddingEMA, self).__init__()\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        init_bound = 1 / n_embeddings\n",
    "        embedding = torch.Tensor(n_embeddings, embedding_dim)\n",
    "        embedding.uniform_(-init_bound, init_bound)\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "        self.register_buffer(\"ema_count\", torch.zeros(n_embeddings))\n",
    "        self.register_buffer(\"ema_weight\", self.embedding.clone())\n",
    "\n",
    "    def encode(self, x):\n",
    "        M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "\n",
    "        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +\n",
    "                    torch.sum(x_flat ** 2, dim=1, keepdim=True),\n",
    "                                x_flat, self.embedding.t(),\n",
    "                                alpha=-2.0, beta=1.0)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        return quantized, indices.view(x.size(0), x.size(1))\n",
    "    \n",
    "    def retrieve_random_codebook(self, random_indices):\n",
    "        quantized = F.embedding(random_indices, self.embedding)\n",
    "        quantized = quantized.transpose(1, 3)\n",
    "        \n",
    "        return quantized\n",
    "\n",
    "    def forward(self, x):\n",
    "        M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "        \n",
    "        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +\n",
    "                                torch.sum(x_flat ** 2, dim=1, keepdim=True),\n",
    "                                x_flat, self.embedding.t(),\n",
    "                                alpha=-2.0, beta=1.0)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        encodings = F.one_hot(indices, M).float()\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        \n",
    "        if self.training:\n",
    "            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n",
    "            n = torch.sum(self.ema_count)\n",
    "            self.ema_count = (self.ema_count + self.epsilon) / (n + M * self.epsilon) * n\n",
    "\n",
    "            dw = torch.matmul(encodings.t(), x_flat)\n",
    "            self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw\n",
    "            self.embedding = self.ema_weight / self.ema_count.unsqueeze(-1)\n",
    "\n",
    "        codebook_loss = F.mse_loss(x.detach(), quantized)\n",
    "        e_latent_loss = F.mse_loss(x, quantized.detach())\n",
    "        commitment_loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, commitment_loss, codebook_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_sizes=(1, 3, 2, 2), stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        kernel_1, kernel_2, kernel_3, kernel_4 = kernel_sizes\n",
    "        \n",
    "        self.residual_conv_1 = nn.Conv2d(input_dim, hidden_dim, kernel_1, padding=0)\n",
    "        self.residual_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, padding=1)\n",
    "        \n",
    "        self.strided_t_conv_1 = nn.ConvTranspose2d(hidden_dim, hidden_dim, kernel_3, stride, padding=0)\n",
    "        self.strided_t_conv_2 = nn.ConvTranspose2d(hidden_dim, output_dim, kernel_4, stride, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.residual_conv_1(x)\n",
    "        y = y+x\n",
    "        x = F.relu(y)\n",
    "        \n",
    "        y = self.residual_conv_2(x)\n",
    "        y = y+x\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        y = self.strided_t_conv_1(y)\n",
    "        y = self.strided_t_conv_2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Codebook, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.codebook = Codebook\n",
    "        self.decoder = Decoder\n",
    "                \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_quantized, commitment_loss, codebook_loss, perplexity = self.codebook(z)\n",
    "        x_hat = self.decoder(z_quantized)\n",
    "        \n",
    "        return x_hat, commitment_loss, codebook_loss, perplexity\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=hidden_dim)\n",
    "codebook = VQEmbeddingEMA(n_embeddings=n_embeddings, embedding_dim=hidden_dim)\n",
    "decoder = Decoder(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Codebook=codebook, Decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define Loss function (reprod. loss) and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Train Vector Quantized Variational AutoEncoder (VQ-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VQ-VAE...\n",
      "epoch: 1   step: 1   recon_loss: 0.32385456562042236   perplexity:  49.421241760253906 \n",
      "\t\tcommit_loss:  0.0065625375136733055   codebook loss:  0.026250150054693222   total_loss:  0.35666725039482117\n",
      "epoch: 1   step: 51   recon_loss: 0.04821508377790451   perplexity:  29.88054847717285 \n",
      "\t\tcommit_loss:  0.021651318296790123   codebook loss:  0.08660527318716049   total_loss:  0.15647166967391968\n",
      "epoch: 1   step: 101   recon_loss: 0.040474675595760345   perplexity:  48.35503387451172 \n",
      "\t\tcommit_loss:  0.0379059761762619   codebook loss:  0.1516239047050476   total_loss:  0.23000454902648926\n",
      "epoch: 1   step: 151   recon_loss: 0.035965725779533386   perplexity:  62.463623046875 \n",
      "\t\tcommit_loss:  0.03702661395072937   codebook loss:  0.14810645580291748   total_loss:  0.22109879553318024\n",
      "epoch: 1   step: 201   recon_loss: 0.031123753637075424   perplexity:  77.00450897216797 \n",
      "\t\tcommit_loss:  0.04220336675643921   codebook loss:  0.16881346702575684   total_loss:  0.24214059114456177\n",
      "epoch: 1   step: 251   recon_loss: 0.02864379994571209   perplexity:  113.28253936767578 \n",
      "\t\tcommit_loss:  0.04223552346229553   codebook loss:  0.16894209384918213   total_loss:  0.2398214191198349\n",
      "epoch: 1   step: 301   recon_loss: 0.02518518641591072   perplexity:  157.64266967773438 \n",
      "\t\tcommit_loss:  0.04033923149108887   codebook loss:  0.16135692596435547   total_loss:  0.22688134014606476\n",
      "epoch: 1   step: 351   recon_loss: 0.02551724575459957   perplexity:  218.77574157714844 \n",
      "\t\tcommit_loss:  0.038867972791194916   codebook loss:  0.15547189116477966   total_loss:  0.2198571115732193\n",
      "epoch: 2   step: 1   recon_loss: 0.024028733372688293   perplexity:  272.7276916503906 \n",
      "\t\tcommit_loss:  0.0358717180788517   codebook loss:  0.1434868723154068   total_loss:  0.2033873200416565\n",
      "epoch: 2   step: 51   recon_loss: 0.02039686217904091   perplexity:  340.1007080078125 \n",
      "\t\tcommit_loss:  0.03266201913356781   codebook loss:  0.13064807653427124   total_loss:  0.18370695412158966\n",
      "epoch: 2   step: 101   recon_loss: 0.018994808197021484   perplexity:  391.431884765625 \n",
      "\t\tcommit_loss:  0.03231023997068405   codebook loss:  0.1292409598827362   total_loss:  0.18054601550102234\n",
      "epoch: 2   step: 151   recon_loss: 0.018526161089539528   perplexity:  404.6041564941406 \n",
      "\t\tcommit_loss:  0.032293885946273804   codebook loss:  0.12917554378509521   total_loss:  0.179995596408844\n",
      "epoch: 2   step: 201   recon_loss: 0.018486827611923218   perplexity:  466.7431640625 \n",
      "\t\tcommit_loss:  0.03621715307235718   codebook loss:  0.1448686122894287   total_loss:  0.1995725929737091\n",
      "epoch: 2   step: 251   recon_loss: 0.01888023130595684   perplexity:  464.6969909667969 \n",
      "\t\tcommit_loss:  0.037989336997270584   codebook loss:  0.15195734798908234   total_loss:  0.2088269144296646\n",
      "epoch: 2   step: 301   recon_loss: 0.016956491395831108   perplexity:  474.8081970214844 \n",
      "\t\tcommit_loss:  0.036788083612918854   codebook loss:  0.14715233445167542   total_loss:  0.20089691877365112\n",
      "epoch: 2   step: 351   recon_loss: 0.01726069673895836   perplexity:  498.2001037597656 \n",
      "\t\tcommit_loss:  0.03879997506737709   codebook loss:  0.15519990026950836   total_loss:  0.2112605720758438\n",
      "epoch: 3   step: 1   recon_loss: 0.015940841287374496   perplexity:  486.0714416503906 \n",
      "\t\tcommit_loss:  0.036402106285095215   codebook loss:  0.14560842514038086   total_loss:  0.19795137643814087\n",
      "epoch: 3   step: 51   recon_loss: 0.015009507536888123   perplexity:  493.8724060058594 \n",
      "\t\tcommit_loss:  0.03447013348340988   codebook loss:  0.13788053393363953   total_loss:  0.18736016750335693\n",
      "epoch: 3   step: 101   recon_loss: 0.015056788921356201   perplexity:  497.57427978515625 \n",
      "\t\tcommit_loss:  0.035369984805583954   codebook loss:  0.14147993922233582   total_loss:  0.19190672039985657\n",
      "epoch: 3   step: 151   recon_loss: 0.014592912048101425   perplexity:  493.4322204589844 \n",
      "\t\tcommit_loss:  0.03388320654630661   codebook loss:  0.13553282618522644   total_loss:  0.18400894105434418\n",
      "epoch: 3   step: 201   recon_loss: 0.01512540876865387   perplexity:  517.2008056640625 \n",
      "\t\tcommit_loss:  0.036635324358940125   codebook loss:  0.1465412974357605   total_loss:  0.1983020305633545\n",
      "epoch: 3   step: 251   recon_loss: 0.015404105186462402   perplexity:  502.8136291503906 \n",
      "\t\tcommit_loss:  0.03815961256623268   codebook loss:  0.15263845026493073   total_loss:  0.2062021642923355\n",
      "epoch: 3   step: 301   recon_loss: 0.014066115021705627   perplexity:  490.3357238769531 \n",
      "\t\tcommit_loss:  0.035210225731134415   codebook loss:  0.14084090292453766   total_loss:  0.1901172399520874\n",
      "epoch: 3   step: 351   recon_loss: 0.014487202279269695   perplexity:  510.88037109375 \n",
      "\t\tcommit_loss:  0.03494928032159805   codebook loss:  0.1397971212863922   total_loss:  0.18923360109329224\n",
      "epoch: 4   step: 1   recon_loss: 0.013584060594439507   perplexity:  493.0735168457031 \n",
      "\t\tcommit_loss:  0.03361736610531807   codebook loss:  0.13446946442127228   total_loss:  0.1816708892583847\n",
      "epoch: 4   step: 51   recon_loss: 0.012547391466796398   perplexity:  489.9937744140625 \n",
      "\t\tcommit_loss:  0.03187141567468643   codebook loss:  0.12748566269874573   total_loss:  0.17190447449684143\n",
      "epoch: 4   step: 101   recon_loss: 0.013453163206577301   perplexity:  483.3902587890625 \n",
      "\t\tcommit_loss:  0.03388352692127228   codebook loss:  0.1355341076850891   total_loss:  0.1828708052635193\n",
      "epoch: 4   step: 151   recon_loss: 0.012126080691814423   perplexity:  495.79022216796875 \n",
      "\t\tcommit_loss:  0.03269198536872864   codebook loss:  0.13076794147491455   total_loss:  0.1755860149860382\n",
      "epoch: 4   step: 201   recon_loss: 0.013737003318965435   perplexity:  515.7623291015625 \n",
      "\t\tcommit_loss:  0.03634559363126755   codebook loss:  0.1453823745250702   total_loss:  0.19546496868133545\n",
      "epoch: 4   step: 251   recon_loss: 0.013863999396562576   perplexity:  529.604248046875 \n",
      "\t\tcommit_loss:  0.036065105348825455   codebook loss:  0.14426042139530182   total_loss:  0.19418951869010925\n",
      "epoch: 4   step: 301   recon_loss: 0.013753212988376617   perplexity:  534.61865234375 \n",
      "\t\tcommit_loss:  0.03728592395782471   codebook loss:  0.14914369583129883   total_loss:  0.20018282532691956\n",
      "epoch: 4   step: 351   recon_loss: 0.014220180921256542   perplexity:  544.1370239257812 \n",
      "\t\tcommit_loss:  0.03736484795808792   codebook loss:  0.14945939183235168   total_loss:  0.20104442536830902\n",
      "epoch: 5   step: 1   recon_loss: 0.012495972216129303   perplexity:  509.51898193359375 \n",
      "\t\tcommit_loss:  0.03460917994379997   codebook loss:  0.1384367197751999   total_loss:  0.18554186820983887\n",
      "epoch: 5   step: 51   recon_loss: 0.012806053273379803   perplexity:  526.915771484375 \n",
      "\t\tcommit_loss:  0.0350489616394043   codebook loss:  0.1401958465576172   total_loss:  0.18805086612701416\n",
      "epoch: 5   step: 101   recon_loss: 0.012509308755397797   perplexity:  522.6754760742188 \n",
      "\t\tcommit_loss:  0.03469499945640564   codebook loss:  0.13877999782562256   total_loss:  0.1859843134880066\n",
      "epoch: 5   step: 151   recon_loss: 0.012520815245807171   perplexity:  528.9762573242188 \n",
      "\t\tcommit_loss:  0.03448445349931717   codebook loss:  0.13793781399726868   total_loss:  0.1849430799484253\n",
      "epoch: 5   step: 201   recon_loss: 0.012765318155288696   perplexity:  535.17578125 \n",
      "\t\tcommit_loss:  0.03612854331731796   codebook loss:  0.14451417326927185   total_loss:  0.1934080421924591\n",
      "epoch: 5   step: 251   recon_loss: 0.012303217314183712   perplexity:  523.9600830078125 \n",
      "\t\tcommit_loss:  0.03414171189069748   codebook loss:  0.13656684756278992   total_loss:  0.18301177024841309\n",
      "epoch: 5   step: 301   recon_loss: 0.01341340597718954   perplexity:  542.3786010742188 \n",
      "\t\tcommit_loss:  0.036618851125240326   codebook loss:  0.1464754045009613   total_loss:  0.19650766253471375\n",
      "epoch: 5   step: 351   recon_loss: 0.011933945119380951   perplexity:  528.3057861328125 \n",
      "\t\tcommit_loss:  0.03315253555774689   codebook loss:  0.13261014223098755   total_loss:  0.1776966154575348\n",
      "epoch: 6   step: 1   recon_loss: 0.012278346344828606   perplexity:  537.7088623046875 \n",
      "\t\tcommit_loss:  0.03445806726813316   codebook loss:  0.13783226907253265   total_loss:  0.18456867337226868\n",
      "epoch: 6   step: 51   recon_loss: 0.012453276664018631   perplexity:  548.9561767578125 \n",
      "\t\tcommit_loss:  0.034860096871852875   codebook loss:  0.1394403874874115   total_loss:  0.1867537647485733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6   step: 101   recon_loss: 0.011985739693045616   perplexity:  536.3685302734375 \n",
      "\t\tcommit_loss:  0.03270594775676727   codebook loss:  0.1308237910270691   total_loss:  0.17551547288894653\n",
      "epoch: 6   step: 151   recon_loss: 0.012983204796910286   perplexity:  537.3038940429688 \n",
      "\t\tcommit_loss:  0.03671018034219742   codebook loss:  0.14684072136878967   total_loss:  0.19653409719467163\n",
      "epoch: 6   step: 201   recon_loss: 0.011857176199555397   perplexity:  539.6919555664062 \n",
      "\t\tcommit_loss:  0.03354458138346672   codebook loss:  0.13417832553386688   total_loss:  0.17958009243011475\n",
      "epoch: 6   step: 251   recon_loss: 0.011263872496783733   perplexity:  534.7515258789062 \n",
      "\t\tcommit_loss:  0.0321807786822319   codebook loss:  0.1287231147289276   total_loss:  0.17216776311397552\n",
      "epoch: 6   step: 301   recon_loss: 0.012824351899325848   perplexity:  568.3358154296875 \n",
      "\t\tcommit_loss:  0.0361628532409668   codebook loss:  0.1446514129638672   total_loss:  0.1936386227607727\n",
      "epoch: 6   step: 351   recon_loss: 0.011914185248315334   perplexity:  559.812255859375 \n",
      "\t\tcommit_loss:  0.03491685539484024   codebook loss:  0.13966742157936096   total_loss:  0.1864984631538391\n",
      "epoch: 7   step: 1   recon_loss: 0.012036311440169811   perplexity:  563.9000244140625 \n",
      "\t\tcommit_loss:  0.03405524417757988   codebook loss:  0.13622097671031952   total_loss:  0.18231253325939178\n",
      "epoch: 7   step: 51   recon_loss: 0.01279788464307785   perplexity:  564.3497924804688 \n",
      "\t\tcommit_loss:  0.035924240946769714   codebook loss:  0.14369696378707886   total_loss:  0.19241908192634583\n",
      "epoch: 7   step: 101   recon_loss: 0.012270674109458923   perplexity:  550.7548828125 \n",
      "\t\tcommit_loss:  0.034056082367897034   codebook loss:  0.13622432947158813   total_loss:  0.1825510859489441\n",
      "epoch: 7   step: 151   recon_loss: 0.011120324954390526   perplexity:  543.2322387695312 \n",
      "\t\tcommit_loss:  0.031446922570466995   codebook loss:  0.12578769028186798   total_loss:  0.16835492849349976\n",
      "epoch: 7   step: 201   recon_loss: 0.01155821606516838   perplexity:  554.8681030273438 \n",
      "\t\tcommit_loss:  0.03257107734680176   codebook loss:  0.13028430938720703   total_loss:  0.17441360652446747\n",
      "epoch: 7   step: 251   recon_loss: 0.01151708047837019   perplexity:  560.823486328125 \n",
      "\t\tcommit_loss:  0.03330351412296295   codebook loss:  0.1332140564918518   total_loss:  0.17803464829921722\n",
      "epoch: 7   step: 301   recon_loss: 0.011334829032421112   perplexity:  565.2315063476562 \n",
      "\t\tcommit_loss:  0.032659292221069336   codebook loss:  0.13063716888427734   total_loss:  0.1746312975883484\n",
      "epoch: 7   step: 351   recon_loss: 0.011711088940501213   perplexity:  561.32568359375 \n",
      "\t\tcommit_loss:  0.03303030878305435   codebook loss:  0.1321212351322174   total_loss:  0.17686262726783752\n",
      "epoch: 8   step: 1   recon_loss: 0.01176185067743063   perplexity:  559.5087890625 \n",
      "\t\tcommit_loss:  0.03420143574476242   codebook loss:  0.13680574297904968   total_loss:  0.1827690303325653\n",
      "epoch: 8   step: 51   recon_loss: 0.011096164584159851   perplexity:  548.7075805664062 \n",
      "\t\tcommit_loss:  0.031795434653759   codebook loss:  0.127181738615036   total_loss:  0.17007333040237427\n",
      "epoch: 8   step: 101   recon_loss: 0.011595207266509533   perplexity:  549.0185546875 \n",
      "\t\tcommit_loss:  0.03381919488310814   codebook loss:  0.13527677953243256   total_loss:  0.1806911826133728\n",
      "epoch: 8   step: 151   recon_loss: 0.010699975304305553   perplexity:  555.19970703125 \n",
      "\t\tcommit_loss:  0.030666809529066086   codebook loss:  0.12266723811626434   total_loss:  0.16403402388095856\n",
      "epoch: 8   step: 201   recon_loss: 0.010167635045945644   perplexity:  541.0957641601562 \n",
      "\t\tcommit_loss:  0.029769185930490494   codebook loss:  0.11907674372196198   total_loss:  0.159013569355011\n",
      "epoch: 8   step: 251   recon_loss: 0.010953262448310852   perplexity:  566.1432495117188 \n",
      "\t\tcommit_loss:  0.03252392262220383   codebook loss:  0.1300956904888153   total_loss:  0.1735728681087494\n",
      "epoch: 8   step: 301   recon_loss: 0.011685274541378021   perplexity:  567.1453857421875 \n",
      "\t\tcommit_loss:  0.033909738063812256   codebook loss:  0.13563895225524902   total_loss:  0.1812339723110199\n",
      "epoch: 8   step: 351   recon_loss: 0.010322341695427895   perplexity:  558.251708984375 \n",
      "\t\tcommit_loss:  0.03069474548101425   codebook loss:  0.122778981924057   total_loss:  0.163796067237854\n",
      "epoch: 9   step: 1   recon_loss: 0.011258738115429878   perplexity:  574.4163208007812 \n",
      "\t\tcommit_loss:  0.0325908437371254   codebook loss:  0.1303633749485016   total_loss:  0.1742129623889923\n",
      "epoch: 9   step: 51   recon_loss: 0.010392004624009132   perplexity:  555.3275756835938 \n",
      "\t\tcommit_loss:  0.02945101074874401   codebook loss:  0.11780404299497604   total_loss:  0.1576470583677292\n",
      "epoch: 9   step: 101   recon_loss: 0.011450601741671562   perplexity:  567.07861328125 \n",
      "\t\tcommit_loss:  0.033876385539770126   codebook loss:  0.1355055421590805   total_loss:  0.18083253502845764\n",
      "epoch: 9   step: 151   recon_loss: 0.010497558861970901   perplexity:  563.83203125 \n",
      "\t\tcommit_loss:  0.031328216195106506   codebook loss:  0.12531286478042603   total_loss:  0.16713863611221313\n",
      "epoch: 9   step: 201   recon_loss: 0.01044917106628418   perplexity:  565.998291015625 \n",
      "\t\tcommit_loss:  0.030729500576853752   codebook loss:  0.12291800230741501   total_loss:  0.1640966832637787\n",
      "epoch: 9   step: 251   recon_loss: 0.010019548237323761   perplexity:  550.8241577148438 \n",
      "\t\tcommit_loss:  0.02846408821642399   codebook loss:  0.11385635286569595   total_loss:  0.15233999490737915\n",
      "epoch: 9   step: 301   recon_loss: 0.010050570592284203   perplexity:  561.8221435546875 \n",
      "\t\tcommit_loss:  0.029108207672834396   codebook loss:  0.11643283069133759   total_loss:  0.15559160709381104\n",
      "epoch: 9   step: 351   recon_loss: 0.00987539254128933   perplexity:  548.3233642578125 \n",
      "\t\tcommit_loss:  0.02870451845228672   codebook loss:  0.11481807380914688   total_loss:  0.15339797735214233\n",
      "epoch: 10   step: 1   recon_loss: 0.010764594189822674   perplexity:  562.0868530273438 \n",
      "\t\tcommit_loss:  0.030534256249666214   codebook loss:  0.12213702499866486   total_loss:  0.16343587636947632\n",
      "epoch: 10   step: 51   recon_loss: 0.011605869047343731   perplexity:  581.5111694335938 \n",
      "\t\tcommit_loss:  0.032778605818748474   codebook loss:  0.1311144232749939   total_loss:  0.17549890279769897\n",
      "epoch: 10   step: 101   recon_loss: 0.00945829413831234   perplexity:  567.8757934570312 \n",
      "\t\tcommit_loss:  0.02904457598924637   codebook loss:  0.11617830395698547   total_loss:  0.15468117594718933\n",
      "epoch: 10   step: 151   recon_loss: 0.011392894200980663   perplexity:  579.6126098632812 \n",
      "\t\tcommit_loss:  0.03319947421550751   codebook loss:  0.13279789686203003   total_loss:  0.17739026248455048\n",
      "epoch: 10   step: 201   recon_loss: 0.009908689185976982   perplexity:  578.9110107421875 \n",
      "\t\tcommit_loss:  0.029777390882372856   codebook loss:  0.11910956352949142   total_loss:  0.15879563987255096\n",
      "epoch: 10   step: 251   recon_loss: 0.0105318333953619   perplexity:  587.8885498046875 \n",
      "\t\tcommit_loss:  0.031801655888557434   codebook loss:  0.12720662355422974   total_loss:  0.16954010725021362\n",
      "epoch: 10   step: 301   recon_loss: 0.010809498839080334   perplexity:  590.0127563476562 \n",
      "\t\tcommit_loss:  0.03258635476231575   codebook loss:  0.130345419049263   total_loss:  0.17374128103256226\n",
      "epoch: 10   step: 351   recon_loss: 0.010283324867486954   perplexity:  574.3193969726562 \n",
      "\t\tcommit_loss:  0.031148213893175125   codebook loss:  0.1245928555727005   total_loss:  0.16602438688278198\n",
      "epoch: 11   step: 1   recon_loss: 0.01046567689627409   perplexity:  566.5191650390625 \n",
      "\t\tcommit_loss:  0.02985250949859619   codebook loss:  0.11941003799438477   total_loss:  0.15972822904586792\n",
      "epoch: 11   step: 51   recon_loss: 0.010295109823346138   perplexity:  586.1113891601562 \n",
      "\t\tcommit_loss:  0.030805397778749466   codebook loss:  0.12322159111499786   total_loss:  0.1643221080303192\n",
      "epoch: 11   step: 101   recon_loss: 0.010073846206068993   perplexity:  588.2749633789062 \n",
      "\t\tcommit_loss:  0.02974146604537964   codebook loss:  0.11896586418151855   total_loss:  0.15878117084503174\n",
      "epoch: 11   step: 151   recon_loss: 0.010067395865917206   perplexity:  576.5848388671875 \n",
      "\t\tcommit_loss:  0.02983567863702774   codebook loss:  0.11934271454811096   total_loss:  0.1592457890510559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11   step: 201   recon_loss: 0.010002650320529938   perplexity:  583.0724487304688 \n",
      "\t\tcommit_loss:  0.029295869171619415   codebook loss:  0.11718347668647766   total_loss:  0.15648199617862701\n",
      "epoch: 11   step: 251   recon_loss: 0.011007058434188366   perplexity:  587.8336181640625 \n",
      "\t\tcommit_loss:  0.03199701011180878   codebook loss:  0.1279880404472351   total_loss:  0.17099210619926453\n",
      "epoch: 11   step: 301   recon_loss: 0.010442445985972881   perplexity:  579.60595703125 \n",
      "\t\tcommit_loss:  0.03002035804092884   codebook loss:  0.12008143216371536   total_loss:  0.1605442315340042\n",
      "epoch: 11   step: 351   recon_loss: 0.01022409275174141   perplexity:  572.9700317382812 \n",
      "\t\tcommit_loss:  0.030176343396306038   codebook loss:  0.12070537358522415   total_loss:  0.16110581159591675\n",
      "epoch: 12   step: 1   recon_loss: 0.009681027382612228   perplexity:  592.91259765625 \n",
      "\t\tcommit_loss:  0.029158370569348335   codebook loss:  0.11663348227739334   total_loss:  0.15547287464141846\n",
      "epoch: 12   step: 51   recon_loss: 0.010121874511241913   perplexity:  592.2785034179688 \n",
      "\t\tcommit_loss:  0.030797511339187622   codebook loss:  0.12319004535675049   total_loss:  0.16410943865776062\n",
      "epoch: 12   step: 101   recon_loss: 0.010230598971247673   perplexity:  588.8643188476562 \n",
      "\t\tcommit_loss:  0.030086668208241463   codebook loss:  0.12034667283296585   total_loss:  0.1606639325618744\n",
      "epoch: 12   step: 151   recon_loss: 0.010210799984633923   perplexity:  600.9948120117188 \n",
      "\t\tcommit_loss:  0.03109019063413143   codebook loss:  0.12436076253652573   total_loss:  0.1656617522239685\n",
      "epoch: 12   step: 201   recon_loss: 0.010558165609836578   perplexity:  589.4722290039062 \n",
      "\t\tcommit_loss:  0.030707618221640587   codebook loss:  0.12283047288656235   total_loss:  0.16409626603126526\n",
      "epoch: 12   step: 251   recon_loss: 0.010452497750520706   perplexity:  600.6824951171875 \n",
      "\t\tcommit_loss:  0.02995600923895836   codebook loss:  0.11982403695583344   total_loss:  0.1602325439453125\n",
      "epoch: 12   step: 301   recon_loss: 0.009722328744828701   perplexity:  560.19677734375 \n",
      "\t\tcommit_loss:  0.02923322282731533   codebook loss:  0.11693289130926132   total_loss:  0.15588843822479248\n",
      "epoch: 12   step: 351   recon_loss: 0.010141909122467041   perplexity:  577.5352172851562 \n",
      "\t\tcommit_loss:  0.030550513416528702   codebook loss:  0.12220205366611481   total_loss:  0.16289447247982025\n",
      "epoch: 13   step: 1   recon_loss: 0.009763329289853573   perplexity:  578.0250244140625 \n",
      "\t\tcommit_loss:  0.02874062769114971   codebook loss:  0.11496251076459885   total_loss:  0.15346646308898926\n",
      "epoch: 13   step: 51   recon_loss: 0.010710833594202995   perplexity:  610.1928100585938 \n",
      "\t\tcommit_loss:  0.03179343789815903   codebook loss:  0.1271737515926361   total_loss:  0.16967803239822388\n",
      "epoch: 13   step: 101   recon_loss: 0.010056701488792896   perplexity:  594.5662231445312 \n",
      "\t\tcommit_loss:  0.030126579105854034   codebook loss:  0.12050631642341614   total_loss:  0.1606895923614502\n",
      "epoch: 13   step: 151   recon_loss: 0.01036338321864605   perplexity:  603.9249877929688 \n",
      "\t\tcommit_loss:  0.030926797538995743   codebook loss:  0.12370719015598297   total_loss:  0.16499736905097961\n",
      "epoch: 13   step: 201   recon_loss: 0.010117972269654274   perplexity:  588.7803955078125 \n",
      "\t\tcommit_loss:  0.03031153976917267   codebook loss:  0.12124615907669067   total_loss:  0.16167566180229187\n",
      "epoch: 13   step: 251   recon_loss: 0.009425359778106213   perplexity:  573.6583862304688 \n",
      "\t\tcommit_loss:  0.028422486037015915   codebook loss:  0.11368994414806366   total_loss:  0.15153779089450836\n",
      "epoch: 13   step: 301   recon_loss: 0.009159961715340614   perplexity:  576.8092041015625 \n",
      "\t\tcommit_loss:  0.02732870727777481   codebook loss:  0.10931482911109924   total_loss:  0.14580349624156952\n",
      "epoch: 13   step: 351   recon_loss: 0.009201860055327415   perplexity:  579.0032348632812 \n",
      "\t\tcommit_loss:  0.028614843264222145   codebook loss:  0.11445937305688858   total_loss:  0.15227606892585754\n",
      "epoch: 14   step: 1   recon_loss: 0.009608836844563484   perplexity:  591.4639892578125 \n",
      "\t\tcommit_loss:  0.02827836573123932   codebook loss:  0.11311346292495728   total_loss:  0.15100066363811493\n",
      "epoch: 14   step: 51   recon_loss: 0.009420149028301239   perplexity:  594.0643920898438 \n",
      "\t\tcommit_loss:  0.028770916163921356   codebook loss:  0.11508366465568542   total_loss:  0.15327472984790802\n",
      "epoch: 14   step: 101   recon_loss: 0.009291156195104122   perplexity:  600.572509765625 \n",
      "\t\tcommit_loss:  0.027848049998283386   codebook loss:  0.11139219999313354   total_loss:  0.14853140711784363\n",
      "epoch: 14   step: 151   recon_loss: 0.009925556369125843   perplexity:  596.19384765625 \n",
      "\t\tcommit_loss:  0.029652724042534828   codebook loss:  0.11861089617013931   total_loss:  0.15818917751312256\n",
      "epoch: 14   step: 201   recon_loss: 0.00984701607376337   perplexity:  598.140380859375 \n",
      "\t\tcommit_loss:  0.029944438487291336   codebook loss:  0.11977775394916534   total_loss:  0.15956920385360718\n",
      "epoch: 14   step: 251   recon_loss: 0.010171616449952126   perplexity:  606.4971313476562 \n",
      "\t\tcommit_loss:  0.030502019450068474   codebook loss:  0.1220080778002739   total_loss:  0.1626817137002945\n",
      "epoch: 14   step: 301   recon_loss: 0.00961005873978138   perplexity:  604.673583984375 \n",
      "\t\tcommit_loss:  0.028319939970970154   codebook loss:  0.11327975988388062   total_loss:  0.151209756731987\n",
      "epoch: 14   step: 351   recon_loss: 0.009858231991529465   perplexity:  610.4925537109375 \n",
      "\t\tcommit_loss:  0.02836422622203827   codebook loss:  0.11345690488815308   total_loss:  0.1516793668270111\n",
      "epoch: 15   step: 1   recon_loss: 0.008978989906609058   perplexity:  593.4963989257812 \n",
      "\t\tcommit_loss:  0.02627493441104889   codebook loss:  0.10509973764419556   total_loss:  0.14035366475582123\n",
      "epoch: 15   step: 51   recon_loss: 0.009934000670909882   perplexity:  604.9689331054688 \n",
      "\t\tcommit_loss:  0.028498288244009018   codebook loss:  0.11399315297603607   total_loss:  0.15242543816566467\n",
      "epoch: 15   step: 101   recon_loss: 0.010341217741370201   perplexity:  612.6885375976562 \n",
      "\t\tcommit_loss:  0.029432369396090508   codebook loss:  0.11772947758436203   total_loss:  0.15750306844711304\n",
      "epoch: 15   step: 151   recon_loss: 0.009171782061457634   perplexity:  583.418701171875 \n",
      "\t\tcommit_loss:  0.027125846594572067   codebook loss:  0.10850338637828827   total_loss:  0.14480102062225342\n",
      "epoch: 15   step: 201   recon_loss: 0.009308898821473122   perplexity:  606.4789428710938 \n",
      "\t\tcommit_loss:  0.02718253806233406   codebook loss:  0.10873015224933624   total_loss:  0.14522159099578857\n",
      "epoch: 15   step: 251   recon_loss: 0.008998670615255833   perplexity:  605.1564331054688 \n",
      "\t\tcommit_loss:  0.02627350389957428   codebook loss:  0.10509401559829712   total_loss:  0.14036619663238525\n",
      "epoch: 15   step: 301   recon_loss: 0.009418241679668427   perplexity:  612.78173828125 \n",
      "\t\tcommit_loss:  0.028406593948602676   codebook loss:  0.1136263757944107   total_loss:  0.1514512151479721\n",
      "epoch: 15   step: 351   recon_loss: 0.009639992378652096   perplexity:  624.6181030273438 \n",
      "\t\tcommit_loss:  0.02908376231789589   codebook loss:  0.11633504927158356   total_loss:  0.15505880117416382\n",
      "epoch: 16   step: 1   recon_loss: 0.009270026348531246   perplexity:  598.674560546875 \n",
      "\t\tcommit_loss:  0.02862025424838066   codebook loss:  0.11448101699352264   total_loss:  0.15237130224704742\n",
      "epoch: 16   step: 51   recon_loss: 0.00951410736888647   perplexity:  613.4401245117188 \n",
      "\t\tcommit_loss:  0.028233487159013748   codebook loss:  0.11293394863605499   total_loss:  0.1506815403699875\n",
      "epoch: 16   step: 101   recon_loss: 0.008585698902606964   perplexity:  593.5784912109375 \n",
      "\t\tcommit_loss:  0.02669915370643139   codebook loss:  0.10679661482572556   total_loss:  0.14208146929740906\n",
      "epoch: 16   step: 151   recon_loss: 0.009284566156566143   perplexity:  595.3455200195312 \n",
      "\t\tcommit_loss:  0.028162039816379547   codebook loss:  0.11264815926551819   total_loss:  0.15009476244449615\n",
      "epoch: 16   step: 201   recon_loss: 0.009639880619943142   perplexity:  614.66552734375 \n",
      "\t\tcommit_loss:  0.028640402480959892   codebook loss:  0.11456160992383957   total_loss:  0.15284189581871033\n",
      "epoch: 16   step: 251   recon_loss: 0.009336847811937332   perplexity:  628.8558349609375 \n",
      "\t\tcommit_loss:  0.02825181744992733   codebook loss:  0.11300726979970932   total_loss:  0.15059593319892883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16   step: 301   recon_loss: 0.008758357726037502   perplexity:  594.69189453125 \n",
      "\t\tcommit_loss:  0.02623741514980793   codebook loss:  0.10494966059923172   total_loss:  0.13994543254375458\n",
      "epoch: 16   step: 351   recon_loss: 0.008988333866000175   perplexity:  607.6864013671875 \n",
      "\t\tcommit_loss:  0.026969151571393013   codebook loss:  0.10787660628557205   total_loss:  0.14383408427238464\n",
      "epoch: 17   step: 1   recon_loss: 0.009179063141345978   perplexity:  621.39697265625 \n",
      "\t\tcommit_loss:  0.027466967701911926   codebook loss:  0.1098678708076477   total_loss:  0.1465139091014862\n",
      "epoch: 17   step: 51   recon_loss: 0.009396265260875225   perplexity:  623.3541870117188 \n",
      "\t\tcommit_loss:  0.027633408084511757   codebook loss:  0.11053363233804703   total_loss:  0.14756330847740173\n",
      "epoch: 17   step: 101   recon_loss: 0.009094013832509518   perplexity:  623.0403442382812 \n",
      "\t\tcommit_loss:  0.027552243322134018   codebook loss:  0.11020897328853607   total_loss:  0.14685523509979248\n",
      "epoch: 17   step: 151   recon_loss: 0.008920661173760891   perplexity:  609.919921875 \n",
      "\t\tcommit_loss:  0.027143362909555435   codebook loss:  0.10857345163822174   total_loss:  0.14463748037815094\n",
      "epoch: 17   step: 201   recon_loss: 0.008830761536955833   perplexity:  610.499267578125 \n",
      "\t\tcommit_loss:  0.026084784418344498   codebook loss:  0.10433913767337799   total_loss:  0.13925468921661377\n",
      "epoch: 17   step: 251   recon_loss: 0.009798161685466766   perplexity:  612.3693237304688 \n",
      "\t\tcommit_loss:  0.028680816292762756   codebook loss:  0.11472326517105103   total_loss:  0.15320223569869995\n",
      "epoch: 17   step: 301   recon_loss: 0.009191423654556274   perplexity:  616.3119506835938 \n",
      "\t\tcommit_loss:  0.026830041781067848   codebook loss:  0.10732016712427139   total_loss:  0.14334163069725037\n",
      "epoch: 17   step: 351   recon_loss: 0.009221944957971573   perplexity:  619.23681640625 \n",
      "\t\tcommit_loss:  0.027362225577235222   codebook loss:  0.10944890230894089   total_loss:  0.14603307843208313\n",
      "epoch: 18   step: 1   recon_loss: 0.009306485764682293   perplexity:  615.5325317382812 \n",
      "\t\tcommit_loss:  0.027564648538827896   codebook loss:  0.11025859415531158   total_loss:  0.14712972939014435\n",
      "epoch: 18   step: 51   recon_loss: 0.009785491973161697   perplexity:  621.8347778320312 \n",
      "\t\tcommit_loss:  0.028442252427339554   codebook loss:  0.11376900970935822   total_loss:  0.15199676156044006\n",
      "epoch: 18   step: 101   recon_loss: 0.009452959522604942   perplexity:  630.2769775390625 \n",
      "\t\tcommit_loss:  0.027821272611618042   codebook loss:  0.11128509044647217   total_loss:  0.1485593318939209\n",
      "epoch: 18   step: 151   recon_loss: 0.008431471884250641   perplexity:  602.5985717773438 \n",
      "\t\tcommit_loss:  0.02556505985558033   codebook loss:  0.10226023942232132   total_loss:  0.13625676929950714\n",
      "epoch: 18   step: 201   recon_loss: 0.009294891729950905   perplexity:  627.166259765625 \n",
      "\t\tcommit_loss:  0.02781365066766739   codebook loss:  0.11125460267066956   total_loss:  0.1483631432056427\n",
      "epoch: 18   step: 251   recon_loss: 0.008741186931729317   perplexity:  614.7302856445312 \n",
      "\t\tcommit_loss:  0.026085082441568375   codebook loss:  0.1043403297662735   total_loss:  0.13916659355163574\n",
      "epoch: 18   step: 301   recon_loss: 0.009388089179992676   perplexity:  608.9113159179688 \n",
      "\t\tcommit_loss:  0.027560995891690254   codebook loss:  0.11024398356676102   total_loss:  0.1471930742263794\n",
      "epoch: 18   step: 351   recon_loss: 0.00932256504893303   perplexity:  627.304443359375 \n",
      "\t\tcommit_loss:  0.027461854740977287   codebook loss:  0.10984741896390915   total_loss:  0.14663183689117432\n",
      "epoch: 19   step: 1   recon_loss: 0.009687363170087337   perplexity:  637.8147583007812 \n",
      "\t\tcommit_loss:  0.028085066005587578   codebook loss:  0.11234026402235031   total_loss:  0.15011268854141235\n",
      "epoch: 19   step: 51   recon_loss: 0.009274596348404884   perplexity:  628.0880126953125 \n",
      "\t\tcommit_loss:  0.0272737555205822   codebook loss:  0.1090950220823288   total_loss:  0.14564338326454163\n",
      "epoch: 19   step: 101   recon_loss: 0.008705304935574532   perplexity:  633.49462890625 \n",
      "\t\tcommit_loss:  0.02584279328584671   codebook loss:  0.10337117314338684   total_loss:  0.13791927695274353\n",
      "epoch: 19   step: 151   recon_loss: 0.00835480447858572   perplexity:  611.1837158203125 \n",
      "\t\tcommit_loss:  0.025828372687101364   codebook loss:  0.10331349074840546   total_loss:  0.13749666512012482\n",
      "epoch: 19   step: 201   recon_loss: 0.008540825918316841   perplexity:  614.7405395507812 \n",
      "\t\tcommit_loss:  0.025122597813606262   codebook loss:  0.10049039125442505   total_loss:  0.134153813123703\n",
      "epoch: 19   step: 251   recon_loss: 0.008740542456507683   perplexity:  613.239501953125 \n",
      "\t\tcommit_loss:  0.025643909350037575   codebook loss:  0.1025756374001503   total_loss:  0.13696008920669556\n",
      "epoch: 19   step: 301   recon_loss: 0.009033952839672565   perplexity:  616.9944458007812 \n",
      "\t\tcommit_loss:  0.026593860238790512   codebook loss:  0.10637544095516205   total_loss:  0.14200325310230255\n",
      "epoch: 19   step: 351   recon_loss: 0.008985835127532482   perplexity:  627.2772216796875 \n",
      "\t\tcommit_loss:  0.027323830872774124   codebook loss:  0.1092953234910965   total_loss:  0.14560499787330627\n",
      "epoch: 20   step: 1   recon_loss: 0.009244032204151154   perplexity:  621.0518798828125 \n",
      "\t\tcommit_loss:  0.0274212509393692   codebook loss:  0.1096850037574768   total_loss:  0.14635029435157776\n",
      "epoch: 20   step: 51   recon_loss: 0.009108232334256172   perplexity:  630.306396484375 \n",
      "\t\tcommit_loss:  0.02768087200820446   codebook loss:  0.11072348803281784   total_loss:  0.14751258492469788\n",
      "epoch: 20   step: 101   recon_loss: 0.00880790501832962   perplexity:  625.7676391601562 \n",
      "\t\tcommit_loss:  0.025392631068825722   codebook loss:  0.10157052427530289   total_loss:  0.13577106595039368\n",
      "epoch: 20   step: 151   recon_loss: 0.00880841352045536   perplexity:  614.6127319335938 \n",
      "\t\tcommit_loss:  0.02600584179162979   codebook loss:  0.10402336716651917   total_loss:  0.13883762061595917\n",
      "epoch: 20   step: 201   recon_loss: 0.009285529144108295   perplexity:  636.6758422851562 \n",
      "\t\tcommit_loss:  0.02758808806538582   codebook loss:  0.11035235226154327   total_loss:  0.1472259759902954\n",
      "epoch: 20   step: 251   recon_loss: 0.008607462979853153   perplexity:  622.9952392578125 \n",
      "\t\tcommit_loss:  0.02547409012913704   codebook loss:  0.10189636051654816   total_loss:  0.13597790896892548\n",
      "epoch: 20   step: 301   recon_loss: 0.00900387205183506   perplexity:  604.9602661132812 \n",
      "\t\tcommit_loss:  0.025846412405371666   codebook loss:  0.10338564962148666   total_loss:  0.1382359266281128\n",
      "epoch: 20   step: 351   recon_loss: 0.008669307455420494   perplexity:  625.51708984375 \n",
      "\t\tcommit_loss:  0.025561831891536713   codebook loss:  0.10224732756614685   total_loss:  0.1364784687757492\n",
      "epoch: 21   step: 1   recon_loss: 0.008756169117987156   perplexity:  636.8464965820312 \n",
      "\t\tcommit_loss:  0.025609781965613365   codebook loss:  0.10243912786245346   total_loss:  0.13680508732795715\n",
      "epoch: 21   step: 51   recon_loss: 0.009118447080254555   perplexity:  638.5508422851562 \n",
      "\t\tcommit_loss:  0.027080943807959557   codebook loss:  0.10832377523183823   total_loss:  0.14452317357063293\n",
      "epoch: 21   step: 101   recon_loss: 0.008776457980275154   perplexity:  607.976806640625 \n",
      "\t\tcommit_loss:  0.02566595748066902   codebook loss:  0.10266382992267609   total_loss:  0.13710623979568481\n",
      "epoch: 21   step: 151   recon_loss: 0.009157414548099041   perplexity:  650.1544799804688 \n",
      "\t\tcommit_loss:  0.02752552554011345   codebook loss:  0.1101021021604538   total_loss:  0.14678505063056946\n",
      "epoch: 21   step: 201   recon_loss: 0.008864269591867924   perplexity:  641.0021362304688 \n",
      "\t\tcommit_loss:  0.026845987886190414   codebook loss:  0.10738395154476166   total_loss:  0.14309421181678772\n",
      "epoch: 21   step: 251   recon_loss: 0.008125333115458488   perplexity:  609.0767822265625 \n",
      "\t\tcommit_loss:  0.024256259202957153   codebook loss:  0.09702503681182861   total_loss:  0.1294066309928894\n",
      "epoch: 21   step: 301   recon_loss: 0.00863941852003336   perplexity:  629.8148803710938 \n",
      "\t\tcommit_loss:  0.02588682621717453   codebook loss:  0.10354730486869812   total_loss:  0.13807354867458344\n",
      "epoch: 21   step: 351   recon_loss: 0.009082082659006119   perplexity:  629.5182495117188 \n",
      "\t\tcommit_loss:  0.026729773730039597   codebook loss:  0.10691909492015839   total_loss:  0.1427309513092041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22   step: 1   recon_loss: 0.008795980364084244   perplexity:  640.1019287109375 \n",
      "\t\tcommit_loss:  0.025962093845009804   codebook loss:  0.10384837538003922   total_loss:  0.138606458902359\n",
      "epoch: 22   step: 51   recon_loss: 0.008641945198178291   perplexity:  645.0164794921875 \n",
      "\t\tcommit_loss:  0.02631344646215439   codebook loss:  0.10525378584861755   total_loss:  0.1402091681957245\n",
      "epoch: 22   step: 101   recon_loss: 0.00819161906838417   perplexity:  635.3233032226562 \n",
      "\t\tcommit_loss:  0.02474459819495678   codebook loss:  0.09897839277982712   total_loss:  0.13191461563110352\n",
      "epoch: 22   step: 151   recon_loss: 0.008440833538770676   perplexity:  625.526611328125 \n",
      "\t\tcommit_loss:  0.024980179965496063   codebook loss:  0.09992071986198425   total_loss:  0.1333417296409607\n",
      "epoch: 22   step: 201   recon_loss: 0.009752742946147919   perplexity:  639.1067504882812 \n",
      "\t\tcommit_loss:  0.028252556920051575   codebook loss:  0.1130102276802063   total_loss:  0.1510155200958252\n",
      "epoch: 22   step: 251   recon_loss: 0.009161657653748989   perplexity:  644.159912109375 \n",
      "\t\tcommit_loss:  0.02599373832345009   codebook loss:  0.10397495329380035   total_loss:  0.1391303539276123\n",
      "epoch: 22   step: 301   recon_loss: 0.00893380492925644   perplexity:  639.2470092773438 \n",
      "\t\tcommit_loss:  0.02660016342997551   codebook loss:  0.10640065371990204   total_loss:  0.1419346183538437\n",
      "epoch: 22   step: 351   recon_loss: 0.008729594759643078   perplexity:  642.1312866210938 \n",
      "\t\tcommit_loss:  0.026136817410588264   codebook loss:  0.10454726964235306   total_loss:  0.13941368460655212\n",
      "epoch: 23   step: 1   recon_loss: 0.00922742672264576   perplexity:  652.6369018554688 \n",
      "\t\tcommit_loss:  0.027236975729465485   codebook loss:  0.10894790291786194   total_loss:  0.14541229605674744\n",
      "epoch: 23   step: 51   recon_loss: 0.008547819219529629   perplexity:  636.2060546875 \n",
      "\t\tcommit_loss:  0.02592691034078598   codebook loss:  0.10370764136314392   total_loss:  0.1381823718547821\n",
      "epoch: 23   step: 101   recon_loss: 0.007788907736539841   perplexity:  622.6946411132812 \n",
      "\t\tcommit_loss:  0.023209217935800552   codebook loss:  0.09283687174320221   total_loss:  0.1238349974155426\n",
      "epoch: 23   step: 151   recon_loss: 0.00881235022097826   perplexity:  649.4359741210938 \n",
      "\t\tcommit_loss:  0.02743135392665863   codebook loss:  0.10972541570663452   total_loss:  0.14596912264823914\n",
      "epoch: 23   step: 201   recon_loss: 0.008407697081565857   perplexity:  641.6571655273438 \n",
      "\t\tcommit_loss:  0.024321144446730614   codebook loss:  0.09728457778692245   total_loss:  0.13001342117786407\n",
      "epoch: 23   step: 251   recon_loss: 0.008913863450288773   perplexity:  653.604248046875 \n",
      "\t\tcommit_loss:  0.02683325484395027   codebook loss:  0.10733301937580109   total_loss:  0.14308014512062073\n",
      "epoch: 23   step: 301   recon_loss: 0.008776682429015636   perplexity:  634.4598999023438 \n",
      "\t\tcommit_loss:  0.025568587705492973   codebook loss:  0.1022743508219719   total_loss:  0.13661962747573853\n",
      "epoch: 23   step: 351   recon_loss: 0.008658207952976227   perplexity:  639.7904052734375 \n",
      "\t\tcommit_loss:  0.025926103815436363   codebook loss:  0.10370441526174545   total_loss:  0.1382887363433838\n",
      "epoch: 24   step: 1   recon_loss: 0.009030863642692566   perplexity:  652.3715209960938 \n",
      "\t\tcommit_loss:  0.026550186797976494   codebook loss:  0.10620074719190598   total_loss:  0.14178180694580078\n",
      "epoch: 24   step: 51   recon_loss: 0.00770588917657733   perplexity:  616.3201904296875 \n",
      "\t\tcommit_loss:  0.02298104390501976   codebook loss:  0.09192417562007904   total_loss:  0.12261110544204712\n",
      "epoch: 24   step: 101   recon_loss: 0.008211923763155937   perplexity:  643.4501342773438 \n",
      "\t\tcommit_loss:  0.02473689615726471   codebook loss:  0.09894758462905884   total_loss:  0.13189640641212463\n",
      "epoch: 24   step: 151   recon_loss: 0.008508104830980301   perplexity:  645.3568115234375 \n",
      "\t\tcommit_loss:  0.025483526289463043   codebook loss:  0.10193410515785217   total_loss:  0.13592574000358582\n",
      "epoch: 24   step: 201   recon_loss: 0.00841030478477478   perplexity:  647.6148681640625 \n",
      "\t\tcommit_loss:  0.025340735912322998   codebook loss:  0.10136294364929199   total_loss:  0.13511398434638977\n",
      "epoch: 24   step: 251   recon_loss: 0.008336607366800308   perplexity:  634.5164794921875 \n",
      "\t\tcommit_loss:  0.02515256032347679   codebook loss:  0.10061024129390717   total_loss:  0.13409940898418427\n",
      "epoch: 24   step: 301   recon_loss: 0.00892284233123064   perplexity:  651.082763671875 \n",
      "\t\tcommit_loss:  0.02590649202466011   codebook loss:  0.10362596809864044   total_loss:  0.13845530152320862\n",
      "epoch: 24   step: 351   recon_loss: 0.008149830624461174   perplexity:  633.1259155273438 \n",
      "\t\tcommit_loss:  0.024346541613340378   codebook loss:  0.09738616645336151   total_loss:  0.1298825442790985\n",
      "epoch: 25   step: 1   recon_loss: 0.008272388949990273   perplexity:  636.0993041992188 \n",
      "\t\tcommit_loss:  0.02513498067855835   codebook loss:  0.1005399227142334   total_loss:  0.13394728302955627\n",
      "epoch: 25   step: 51   recon_loss: 0.007869618013501167   perplexity:  634.8944702148438 \n",
      "\t\tcommit_loss:  0.02371295541524887   codebook loss:  0.09485182166099548   total_loss:  0.12643438577651978\n",
      "epoch: 25   step: 101   recon_loss: 0.008493859320878983   perplexity:  652.8348388671875 \n",
      "\t\tcommit_loss:  0.02529091387987137   codebook loss:  0.10116365551948547   total_loss:  0.13494843244552612\n",
      "epoch: 25   step: 151   recon_loss: 0.008725937455892563   perplexity:  642.7402954101562 \n",
      "\t\tcommit_loss:  0.025546275079250336   codebook loss:  0.10218510031700134   total_loss:  0.13645730912685394\n",
      "epoch: 25   step: 201   recon_loss: 0.008301873691380024   perplexity:  649.3239135742188 \n",
      "\t\tcommit_loss:  0.024871356785297394   codebook loss:  0.09948542714118958   total_loss:  0.13265866041183472\n",
      "epoch: 25   step: 251   recon_loss: 0.008402302861213684   perplexity:  647.7352905273438 \n",
      "\t\tcommit_loss:  0.025121692568063736   codebook loss:  0.10048677027225494   total_loss:  0.13401076197624207\n",
      "epoch: 25   step: 301   recon_loss: 0.00866885669529438   perplexity:  642.8184204101562 \n",
      "\t\tcommit_loss:  0.02595478482544422   codebook loss:  0.10381913930177689   total_loss:  0.1384427845478058\n",
      "epoch: 25   step: 351   recon_loss: 0.008557261899113655   perplexity:  644.8858032226562 \n",
      "\t\tcommit_loss:  0.025471430271863937   codebook loss:  0.10188572108745575   total_loss:  0.1359144151210785\n",
      "epoch: 26   step: 1   recon_loss: 0.008154474198818207   perplexity:  651.5784301757812 \n",
      "\t\tcommit_loss:  0.0249224454164505   codebook loss:  0.099689781665802   total_loss:  0.1327666938304901\n",
      "epoch: 26   step: 51   recon_loss: 0.00873754732310772   perplexity:  661.3894653320312 \n",
      "\t\tcommit_loss:  0.025472089648246765   codebook loss:  0.10188835859298706   total_loss:  0.1360979974269867\n",
      "epoch: 26   step: 101   recon_loss: 0.008381552062928677   perplexity:  644.07666015625 \n",
      "\t\tcommit_loss:  0.024391433224081993   codebook loss:  0.09756573289632797   total_loss:  0.13033871352672577\n",
      "epoch: 26   step: 151   recon_loss: 0.008263135328888893   perplexity:  655.0206909179688 \n",
      "\t\tcommit_loss:  0.024820100516080856   codebook loss:  0.09928040206432343   total_loss:  0.13236364722251892\n",
      "epoch: 26   step: 201   recon_loss: 0.00804873462766409   perplexity:  639.4585571289062 \n",
      "\t\tcommit_loss:  0.024553390219807625   codebook loss:  0.0982135608792305   total_loss:  0.13081568479537964\n",
      "epoch: 26   step: 251   recon_loss: 0.008079909719526768   perplexity:  646.6351318359375 \n",
      "\t\tcommit_loss:  0.023317966610193253   codebook loss:  0.09327186644077301   total_loss:  0.12466974556446075\n",
      "epoch: 26   step: 301   recon_loss: 0.008230473846197128   perplexity:  663.6109008789062 \n",
      "\t\tcommit_loss:  0.025751108303666115   codebook loss:  0.10300443321466446   total_loss:  0.13698601722717285\n",
      "epoch: 26   step: 351   recon_loss: 0.008898635394871235   perplexity:  664.0977172851562 \n",
      "\t\tcommit_loss:  0.026365304365754128   codebook loss:  0.10546121746301651   total_loss:  0.14072516560554504\n",
      "epoch: 27   step: 1   recon_loss: 0.008472040295600891   perplexity:  641.2962036132812 \n",
      "\t\tcommit_loss:  0.024361558258533478   codebook loss:  0.09744623303413391   total_loss:  0.13027983903884888\n",
      "epoch: 27   step: 51   recon_loss: 0.008513806387782097   perplexity:  649.8842163085938 \n",
      "\t\tcommit_loss:  0.025684934109449387   codebook loss:  0.10273973643779755   total_loss:  0.13693848252296448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27   step: 101   recon_loss: 0.009142331779003143   perplexity:  670.5687866210938 \n",
      "\t\tcommit_loss:  0.026896540075540543   codebook loss:  0.10758616030216217   total_loss:  0.14362503588199615\n",
      "epoch: 27   step: 151   recon_loss: 0.008696488104760647   perplexity:  645.0942993164062 \n",
      "\t\tcommit_loss:  0.025842763483524323   codebook loss:  0.10337105393409729   total_loss:  0.13791030645370483\n",
      "epoch: 27   step: 201   recon_loss: 0.008099975995719433   perplexity:  654.7743530273438 \n",
      "\t\tcommit_loss:  0.024892307817935944   codebook loss:  0.09956923127174377   total_loss:  0.13256151974201202\n",
      "epoch: 27   step: 251   recon_loss: 0.008733831346035004   perplexity:  673.514404296875 \n",
      "\t\tcommit_loss:  0.026689447462558746   codebook loss:  0.10675778985023499   total_loss:  0.14218106865882874\n",
      "epoch: 27   step: 301   recon_loss: 0.008362405002117157   perplexity:  661.7427368164062 \n",
      "\t\tcommit_loss:  0.02433408796787262   codebook loss:  0.09733635187149048   total_loss:  0.13003283739089966\n",
      "epoch: 27   step: 351   recon_loss: 0.008541872724890709   perplexity:  663.6326904296875 \n",
      "\t\tcommit_loss:  0.02590849995613098   codebook loss:  0.10363399982452393   total_loss:  0.13808438181877136\n",
      "epoch: 28   step: 1   recon_loss: 0.008216198533773422   perplexity:  650.4239501953125 \n",
      "\t\tcommit_loss:  0.024472689256072044   codebook loss:  0.09789075702428818   total_loss:  0.1305796504020691\n",
      "epoch: 28   step: 51   recon_loss: 0.00800719391554594   perplexity:  638.869140625 \n",
      "\t\tcommit_loss:  0.024196777492761612   codebook loss:  0.09678710997104645   total_loss:  0.12899108231067657\n",
      "epoch: 28   step: 101   recon_loss: 0.008068902418017387   perplexity:  638.2711181640625 \n",
      "\t\tcommit_loss:  0.02397119626402855   codebook loss:  0.0958847850561142   total_loss:  0.12792488932609558\n",
      "epoch: 28   step: 151   recon_loss: 0.00827203132212162   perplexity:  645.729248046875 \n",
      "\t\tcommit_loss:  0.02375350147485733   codebook loss:  0.09501400589942932   total_loss:  0.12703953683376312\n",
      "epoch: 28   step: 201   recon_loss: 0.00779534038156271   perplexity:  649.54345703125 \n",
      "\t\tcommit_loss:  0.02396635338664055   codebook loss:  0.0958654135465622   total_loss:  0.12762710452079773\n",
      "epoch: 28   step: 251   recon_loss: 0.007853428833186626   perplexity:  653.0745849609375 \n",
      "\t\tcommit_loss:  0.02404051274061203   codebook loss:  0.09616205096244812   total_loss:  0.12805598974227905\n",
      "epoch: 28   step: 301   recon_loss: 0.008396907709538937   perplexity:  661.30615234375 \n",
      "\t\tcommit_loss:  0.025112302973866463   codebook loss:  0.10044921189546585   total_loss:  0.13395842909812927\n",
      "epoch: 28   step: 351   recon_loss: 0.007929215207695961   perplexity:  635.84033203125 \n",
      "\t\tcommit_loss:  0.023797105997800827   codebook loss:  0.09518842399120331   total_loss:  0.12691473960876465\n",
      "epoch: 29   step: 1   recon_loss: 0.007612457498908043   perplexity:  637.3428955078125 \n",
      "\t\tcommit_loss:  0.022840257734060287   codebook loss:  0.09136103093624115   total_loss:  0.12181374430656433\n",
      "epoch: 29   step: 51   recon_loss: 0.008455643430352211   perplexity:  661.5093383789062 \n",
      "\t\tcommit_loss:  0.026251768693327904   codebook loss:  0.10500707477331161   total_loss:  0.13971447944641113\n",
      "epoch: 29   step: 101   recon_loss: 0.008024338632822037   perplexity:  640.1388549804688 \n",
      "\t\tcommit_loss:  0.023335881531238556   codebook loss:  0.09334352612495422   total_loss:  0.12470375001430511\n",
      "epoch: 29   step: 151   recon_loss: 0.007876981049776077   perplexity:  651.2650146484375 \n",
      "\t\tcommit_loss:  0.023648519068956375   codebook loss:  0.0945940762758255   total_loss:  0.12611958384513855\n",
      "epoch: 29   step: 201   recon_loss: 0.007973616942763329   perplexity:  651.5491943359375 \n",
      "\t\tcommit_loss:  0.024097153916954994   codebook loss:  0.09638861566781998   total_loss:  0.1284593939781189\n",
      "epoch: 29   step: 251   recon_loss: 0.008657751604914665   perplexity:  660.8434448242188 \n",
      "\t\tcommit_loss:  0.025928612798452377   codebook loss:  0.10371445119380951   total_loss:  0.1383008062839508\n",
      "epoch: 29   step: 301   recon_loss: 0.008286013267934322   perplexity:  662.796875 \n",
      "\t\tcommit_loss:  0.025493791326880455   codebook loss:  0.10197516530752182   total_loss:  0.13575497269630432\n",
      "epoch: 29   step: 351   recon_loss: 0.008554757572710514   perplexity:  660.29443359375 \n",
      "\t\tcommit_loss:  0.0262489914894104   codebook loss:  0.1049959659576416   total_loss:  0.13979971408843994\n",
      "epoch: 30   step: 1   recon_loss: 0.008329248055815697   perplexity:  651.5582275390625 \n",
      "\t\tcommit_loss:  0.025069281458854675   codebook loss:  0.1002771258354187   total_loss:  0.13367566466331482\n",
      "epoch: 30   step: 51   recon_loss: 0.007959071546792984   perplexity:  666.1309814453125 \n",
      "\t\tcommit_loss:  0.02525649592280388   codebook loss:  0.10102598369121552   total_loss:  0.13424155116081238\n",
      "epoch: 30   step: 101   recon_loss: 0.0074034929275512695   perplexity:  644.2025756835938 \n",
      "\t\tcommit_loss:  0.02244335412979126   codebook loss:  0.08977341651916504   total_loss:  0.11962026357650757\n",
      "epoch: 30   step: 151   recon_loss: 0.007909664884209633   perplexity:  655.3128051757812 \n",
      "\t\tcommit_loss:  0.02356644719839096   codebook loss:  0.09426578879356384   total_loss:  0.1257418990135193\n",
      "epoch: 30   step: 201   recon_loss: 0.00794055312871933   perplexity:  658.586669921875 \n",
      "\t\tcommit_loss:  0.024420123547315598   codebook loss:  0.09768049418926239   total_loss:  0.13004116714000702\n",
      "epoch: 30   step: 251   recon_loss: 0.007759186439216137   perplexity:  654.51611328125 \n",
      "\t\tcommit_loss:  0.023042406886816025   codebook loss:  0.0921696275472641   total_loss:  0.12297122180461884\n",
      "epoch: 30   step: 301   recon_loss: 0.008194241672754288   perplexity:  652.8404541015625 \n",
      "\t\tcommit_loss:  0.025211680680513382   codebook loss:  0.10084672272205353   total_loss:  0.1342526376247406\n",
      "epoch: 30   step: 351   recon_loss: 0.008053898811340332   perplexity:  659.8509521484375 \n",
      "\t\tcommit_loss:  0.024763286113739014   codebook loss:  0.09905314445495605   total_loss:  0.1318703293800354\n",
      "epoch: 31   step: 1   recon_loss: 0.008002247661352158   perplexity:  657.2520751953125 \n",
      "\t\tcommit_loss:  0.024426383897662163   codebook loss:  0.09770553559064865   total_loss:  0.13013416528701782\n",
      "epoch: 31   step: 51   recon_loss: 0.007762879133224487   perplexity:  643.5636596679688 \n",
      "\t\tcommit_loss:  0.02332518808543682   codebook loss:  0.09330075234174728   total_loss:  0.12438882142305374\n",
      "epoch: 31   step: 101   recon_loss: 0.008416919969022274   perplexity:  661.2399291992188 \n",
      "\t\tcommit_loss:  0.024832328781485558   codebook loss:  0.09932931512594223   total_loss:  0.13257856667041779\n",
      "epoch: 31   step: 151   recon_loss: 0.008143046870827675   perplexity:  655.93994140625 \n",
      "\t\tcommit_loss:  0.02485075779259205   codebook loss:  0.0994030311703682   total_loss:  0.13239683210849762\n",
      "epoch: 31   step: 201   recon_loss: 0.007967036217451096   perplexity:  646.5368041992188 \n",
      "\t\tcommit_loss:  0.023515870794653893   codebook loss:  0.09406348317861557   total_loss:  0.125546395778656\n",
      "epoch: 31   step: 251   recon_loss: 0.007782452739775181   perplexity:  653.033203125 \n",
      "\t\tcommit_loss:  0.02373450994491577   codebook loss:  0.09493803977966309   total_loss:  0.12645500898361206\n",
      "epoch: 31   step: 301   recon_loss: 0.008010385558009148   perplexity:  653.5064086914062 \n",
      "\t\tcommit_loss:  0.024101339280605316   codebook loss:  0.09640535712242126   total_loss:  0.12851709127426147\n",
      "epoch: 31   step: 351   recon_loss: 0.007474394515156746   perplexity:  654.0615844726562 \n",
      "\t\tcommit_loss:  0.02331990748643875   codebook loss:  0.093279629945755   total_loss:  0.12407393008470535\n",
      "epoch: 32   step: 1   recon_loss: 0.008396822027862072   perplexity:  663.3469848632812 \n",
      "\t\tcommit_loss:  0.02578880451619625   codebook loss:  0.103155218064785   total_loss:  0.13734084367752075\n",
      "epoch: 32   step: 51   recon_loss: 0.008054002188146114   perplexity:  648.6161499023438 \n",
      "\t\tcommit_loss:  0.023892728611826897   codebook loss:  0.09557091444730759   total_loss:  0.12751764059066772\n",
      "epoch: 32   step: 101   recon_loss: 0.008276242762804031   perplexity:  664.355224609375 \n",
      "\t\tcommit_loss:  0.024773268029093742   codebook loss:  0.09909307211637497   total_loss:  0.132142573595047\n",
      "epoch: 32   step: 151   recon_loss: 0.008966462686657906   perplexity:  681.3768920898438 \n",
      "\t\tcommit_loss:  0.027082109823822975   codebook loss:  0.1083284392952919   total_loss:  0.14437700808048248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32   step: 201   recon_loss: 0.008221241645514965   perplexity:  653.1060791015625 \n",
      "\t\tcommit_loss:  0.02501528710126877   codebook loss:  0.10006114840507507   total_loss:  0.13329768180847168\n",
      "epoch: 32   step: 251   recon_loss: 0.007796123623847961   perplexity:  644.71728515625 \n",
      "\t\tcommit_loss:  0.02395198866724968   codebook loss:  0.09580795466899872   total_loss:  0.12755607068538666\n",
      "epoch: 32   step: 301   recon_loss: 0.008702998980879784   perplexity:  672.5176391601562 \n",
      "\t\tcommit_loss:  0.02637225016951561   codebook loss:  0.10548900067806244   total_loss:  0.14056424796581268\n",
      "epoch: 32   step: 351   recon_loss: 0.007927397266030312   perplexity:  658.1292724609375 \n",
      "\t\tcommit_loss:  0.02354181557893753   codebook loss:  0.09416726231575012   total_loss:  0.12563647329807281\n",
      "epoch: 33   step: 1   recon_loss: 0.007859302684664726   perplexity:  664.00146484375 \n",
      "\t\tcommit_loss:  0.023945318534970284   codebook loss:  0.09578127413988113   total_loss:  0.12758588790893555\n",
      "epoch: 33   step: 51   recon_loss: 0.007484640926122665   perplexity:  651.8438110351562 \n",
      "\t\tcommit_loss:  0.0232851505279541   codebook loss:  0.0931406021118164   total_loss:  0.12391039729118347\n",
      "epoch: 33   step: 101   recon_loss: 0.008247653022408485   perplexity:  660.3174438476562 \n",
      "\t\tcommit_loss:  0.024632755666971207   codebook loss:  0.09853102266788483   total_loss:  0.13141143321990967\n",
      "epoch: 33   step: 151   recon_loss: 0.008273620158433914   perplexity:  669.5035400390625 \n",
      "\t\tcommit_loss:  0.025579992681741714   codebook loss:  0.10231997072696686   total_loss:  0.1361735761165619\n",
      "epoch: 33   step: 201   recon_loss: 0.008586528711020947   perplexity:  669.5342407226562 \n",
      "\t\tcommit_loss:  0.02534203790128231   codebook loss:  0.10136815160512924   total_loss:  0.13529671728610992\n",
      "epoch: 33   step: 251   recon_loss: 0.008041216060519218   perplexity:  656.5604248046875 \n",
      "\t\tcommit_loss:  0.02458852156996727   codebook loss:  0.09835408627986908   total_loss:  0.13098382949829102\n",
      "epoch: 33   step: 301   recon_loss: 0.007667844649404287   perplexity:  654.33642578125 \n",
      "\t\tcommit_loss:  0.02357616275548935   codebook loss:  0.0943046510219574   total_loss:  0.12554866075515747\n",
      "epoch: 33   step: 351   recon_loss: 0.00827447697520256   perplexity:  663.9305419921875 \n",
      "\t\tcommit_loss:  0.024718403816223145   codebook loss:  0.09887361526489258   total_loss:  0.13186649978160858\n",
      "epoch: 34   step: 1   recon_loss: 0.008516859263181686   perplexity:  667.2509155273438 \n",
      "\t\tcommit_loss:  0.02506532520055771   codebook loss:  0.10026130080223083   total_loss:  0.13384348154067993\n",
      "epoch: 34   step: 51   recon_loss: 0.008002287708222866   perplexity:  658.7229614257812 \n",
      "\t\tcommit_loss:  0.02429439127445221   codebook loss:  0.09717756509780884   total_loss:  0.12947425246238708\n",
      "epoch: 34   step: 101   recon_loss: 0.008294538594782352   perplexity:  660.4723510742188 \n",
      "\t\tcommit_loss:  0.02390480972826481   codebook loss:  0.09561923891305923   total_loss:  0.12781858444213867\n",
      "epoch: 34   step: 151   recon_loss: 0.00796517077833414   perplexity:  666.6505126953125 \n",
      "\t\tcommit_loss:  0.025025691837072372   codebook loss:  0.10010276734828949   total_loss:  0.13309362530708313\n",
      "epoch: 34   step: 201   recon_loss: 0.007363553158938885   perplexity:  652.8034057617188 \n",
      "\t\tcommit_loss:  0.022763777524232864   codebook loss:  0.09105511009693146   total_loss:  0.12118244171142578\n",
      "epoch: 34   step: 251   recon_loss: 0.00771886482834816   perplexity:  669.699951171875 \n",
      "\t\tcommit_loss:  0.023919664323329926   codebook loss:  0.0956786572933197   total_loss:  0.12731719017028809\n",
      "epoch: 34   step: 301   recon_loss: 0.007884972728788853   perplexity:  645.9999389648438 \n",
      "\t\tcommit_loss:  0.023961534723639488   codebook loss:  0.09584613889455795   total_loss:  0.12769263982772827\n",
      "epoch: 34   step: 351   recon_loss: 0.007818296551704407   perplexity:  659.7974243164062 \n",
      "\t\tcommit_loss:  0.02398032322525978   codebook loss:  0.09592129290103912   total_loss:  0.127719908952713\n",
      "epoch: 35   step: 1   recon_loss: 0.007981967180967331   perplexity:  662.6932373046875 \n",
      "\t\tcommit_loss:  0.024342965334653854   codebook loss:  0.09737186133861542   total_loss:  0.129696786403656\n",
      "epoch: 35   step: 51   recon_loss: 0.008424505591392517   perplexity:  659.7962036132812 \n",
      "\t\tcommit_loss:  0.0248738843947649   codebook loss:  0.0994955375790596   total_loss:  0.13279393315315247\n",
      "epoch: 35   step: 101   recon_loss: 0.008045688271522522   perplexity:  673.1968383789062 \n",
      "\t\tcommit_loss:  0.023934027180075645   codebook loss:  0.09573610872030258   total_loss:  0.1277158260345459\n",
      "epoch: 35   step: 151   recon_loss: 0.008387885056436062   perplexity:  676.4252319335938 \n",
      "\t\tcommit_loss:  0.025965586304664612   codebook loss:  0.10386234521865845   total_loss:  0.1382158100605011\n",
      "epoch: 35   step: 201   recon_loss: 0.0075142597779631615   perplexity:  651.2013549804688 \n",
      "\t\tcommit_loss:  0.023687422275543213   codebook loss:  0.09474968910217285   total_loss:  0.1259513795375824\n",
      "epoch: 35   step: 251   recon_loss: 0.008012531325221062   perplexity:  666.6781616210938 \n",
      "\t\tcommit_loss:  0.02482013963162899   codebook loss:  0.09928055852651596   total_loss:  0.1321132332086563\n",
      "epoch: 35   step: 301   recon_loss: 0.007424089591950178   perplexity:  650.08349609375 \n",
      "\t\tcommit_loss:  0.022945046424865723   codebook loss:  0.09178018569946289   total_loss:  0.12214931845664978\n",
      "epoch: 35   step: 351   recon_loss: 0.007965929806232452   perplexity:  665.6436157226562 \n",
      "\t\tcommit_loss:  0.024646509438753128   codebook loss:  0.09858603775501251   total_loss:  0.1311984807252884\n",
      "epoch: 36   step: 1   recon_loss: 0.008618910796940327   perplexity:  671.0210571289062 \n",
      "\t\tcommit_loss:  0.02559267170727253   codebook loss:  0.10237068682909012   total_loss:  0.13658227026462555\n",
      "epoch: 36   step: 51   recon_loss: 0.007807617075741291   perplexity:  656.8798217773438 \n",
      "\t\tcommit_loss:  0.024033520370721817   codebook loss:  0.09613408148288727   total_loss:  0.1279752254486084\n",
      "epoch: 36   step: 101   recon_loss: 0.007645436562597752   perplexity:  667.983154296875 \n",
      "\t\tcommit_loss:  0.024033598601818085   codebook loss:  0.09613439440727234   total_loss:  0.1278134286403656\n",
      "epoch: 36   step: 151   recon_loss: 0.008195031434297562   perplexity:  668.2166748046875 \n",
      "\t\tcommit_loss:  0.025459960103034973   codebook loss:  0.10183984041213989   total_loss:  0.13549482822418213\n",
      "epoch: 36   step: 201   recon_loss: 0.007886085659265518   perplexity:  669.9202880859375 \n",
      "\t\tcommit_loss:  0.024150937795639038   codebook loss:  0.09660375118255615   total_loss:  0.1286407709121704\n",
      "epoch: 36   step: 251   recon_loss: 0.007760192267596722   perplexity:  647.5518798828125 \n",
      "\t\tcommit_loss:  0.023748397827148438   codebook loss:  0.09499359130859375   total_loss:  0.12650218605995178\n",
      "epoch: 36   step: 301   recon_loss: 0.008286772295832634   perplexity:  668.3441162109375 \n",
      "\t\tcommit_loss:  0.024853870272636414   codebook loss:  0.09941548109054565   total_loss:  0.13255612552165985\n",
      "epoch: 36   step: 351   recon_loss: 0.0077463556081056595   perplexity:  665.767333984375 \n",
      "\t\tcommit_loss:  0.023603960871696472   codebook loss:  0.09441584348678589   total_loss:  0.12576615810394287\n",
      "epoch: 37   step: 1   recon_loss: 0.007444852031767368   perplexity:  654.9151000976562 \n",
      "\t\tcommit_loss:  0.023071538656949997   codebook loss:  0.09228615462779999   total_loss:  0.12280254065990448\n",
      "epoch: 37   step: 51   recon_loss: 0.008203409612178802   perplexity:  655.8861083984375 \n",
      "\t\tcommit_loss:  0.024503229185938835   codebook loss:  0.09801291674375534   total_loss:  0.13071955740451813\n",
      "epoch: 37   step: 101   recon_loss: 0.007125981152057648   perplexity:  635.3178100585938 \n",
      "\t\tcommit_loss:  0.02227635495364666   codebook loss:  0.08910541981458664   total_loss:  0.1185077577829361\n",
      "epoch: 37   step: 151   recon_loss: 0.007844441570341587   perplexity:  664.50634765625 \n",
      "\t\tcommit_loss:  0.0246207807213068   codebook loss:  0.0984831228852272   total_loss:  0.13094834983348846\n",
      "epoch: 37   step: 201   recon_loss: 0.007834307849407196   perplexity:  663.0984497070312 \n",
      "\t\tcommit_loss:  0.023362010717391968   codebook loss:  0.09344804286956787   total_loss:  0.12464436143636703\n",
      "epoch: 37   step: 251   recon_loss: 0.00819716788828373   perplexity:  668.8223266601562 \n",
      "\t\tcommit_loss:  0.024688810110092163   codebook loss:  0.09875524044036865   total_loss:  0.1316412091255188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37   step: 301   recon_loss: 0.007911449298262596   perplexity:  667.14501953125 \n",
      "\t\tcommit_loss:  0.024112755432724953   codebook loss:  0.09645102173089981   total_loss:  0.12847521901130676\n",
      "epoch: 37   step: 351   recon_loss: 0.007632911205291748   perplexity:  664.609619140625 \n",
      "\t\tcommit_loss:  0.022948265075683594   codebook loss:  0.09179306030273438   total_loss:  0.12237423658370972\n",
      "epoch: 38   step: 1   recon_loss: 0.008015411905944347   perplexity:  665.9534301757812 \n",
      "\t\tcommit_loss:  0.025063252076506615   codebook loss:  0.10025300830602646   total_loss:  0.13333167135715485\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training VQ-VAE...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, commitment_loss, codebook_loss, perplexity = model(x)\n",
    "        recon_loss = mse_loss(x_hat, x)\n",
    "        \n",
    "        loss =  recon_loss + commitment_loss + codebook_loss\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % print_step ==0: \n",
    "            print(\"epoch:\", epoch + 1, \"  step:\", batch_idx + 1, \"  recon_loss:\", recon_loss.item(), \"  perplexity: \", perplexity.item(), \n",
    "              \"\\n\\t\\tcommit_loss: \", commitment_loss.item(), \"  codebook loss: \", codebook_loss.item(), \"  total_loss: \", loss.item())\n",
    "    \n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_image(x, postfix):\n",
    "  \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Visualization of {}\".format(postfix))\n",
    "    plt.imshow(np.transpose(make_grid(x.detach().cpu(), padding=2, normalize=True), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "        x_hat, commitment_loss, codebook_loss, perplexity = model(x)\n",
    " \n",
    "        print(\"perplexity: \", perplexity.item(),\"commit_loss: \", commitment_loss.item(), \"  codebook loss: \", codebook_loss.item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample_image(x[:batch_size//2], \"Ground-truth images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample_image(x_hat[:batch_size//2], \"Reconstructed images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Generate samples via random codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_sample_image(codebook, decoder, indices_shape):\n",
    "    \n",
    "    random_indices = torch.floor(torch.rand(indices_shape) * n_embeddings).long().to(DEVICE)\n",
    "    codes = codebook.retrieve_random_codebook(random_indices)\n",
    "    x_hat = decoder(codes.to(DEVICE))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Visualization of Random Codes\")\n",
    "    plt.imshow(np.transpose(make_grid(x_hat.detach().cpu(), padding=2, normalize=True), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_random_sample_image(codebook, decoder, indices_shape=(batch_size//2, img_size[0]//4, img_size[1]//4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
