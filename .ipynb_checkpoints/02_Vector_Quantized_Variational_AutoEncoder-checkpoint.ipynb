{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "dataset_path = '~/datasets'\n",
    "\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "img_size = (32, 32) # (width, height)\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 128\n",
    "n_embeddings= 768\n",
    "output_dim = 3\n",
    "\n",
    "lr = 2e-4\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "print_step = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Step 1. Load (or download) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_dataset = CIFAR10(dataset_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = CIFAR10(dataset_path, transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define our model: Vector Quantized Variational AutoEncoder (VQ-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=(4, 4, 3, 1), stride=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        kernel_1, kernel_2, kernel_3, kernel_4 = kernel_size\n",
    "        \n",
    "        self.strided_conv_1 = nn.Conv2d(input_dim, hidden_dim, kernel_1, stride, padding=1)\n",
    "        self.strided_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, stride, padding=1)\n",
    "        \n",
    "        self.residual_conv_1 = nn.Conv2d(hidden_dim, hidden_dim, kernel_3, padding=1)\n",
    "        self.residual_conv_2 = nn.Conv2d(hidden_dim, output_dim, kernel_4, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.strided_conv_1(x)\n",
    "        x = self.strided_conv_2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        y = self.residual_conv_1(x)\n",
    "        y = y+x\n",
    "        \n",
    "        x = F.relu(y)\n",
    "        y = self.residual_conv_2(x)\n",
    "        y = y+x\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQEmbeddingEMA(nn.Module):\n",
    "    def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25, decay=0.999, epsilon=1e-5):\n",
    "        super(VQEmbeddingEMA, self).__init__()\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        init_bound = 1 / n_embeddings\n",
    "        embedding = torch.Tensor(n_embeddings, embedding_dim)\n",
    "        embedding.uniform_(-init_bound, init_bound)\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "        self.register_buffer(\"ema_count\", torch.zeros(n_embeddings))\n",
    "        self.register_buffer(\"ema_weight\", self.embedding.clone())\n",
    "\n",
    "    def encode(self, x):\n",
    "        M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "\n",
    "        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +\n",
    "                    torch.sum(x_flat ** 2, dim=1, keepdim=True),\n",
    "                                x_flat, self.embedding.t(),\n",
    "                                alpha=-2.0, beta=1.0)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        return quantized, indices.view(x.size(0), x.size(1))\n",
    "    \n",
    "    def retrieve_random_codebook(self, random_indices):\n",
    "        quantized = F.embedding(random_indices, self.embedding)\n",
    "        quantized = quantized.transpose(1, 3)\n",
    "        \n",
    "        return quantized\n",
    "\n",
    "    def forward(self, x):\n",
    "        M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "        \n",
    "        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +\n",
    "                                torch.sum(x_flat ** 2, dim=1, keepdim=True),\n",
    "                                x_flat, self.embedding.t(),\n",
    "                                alpha=-2.0, beta=1.0)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        encodings = F.one_hot(indices, M).float()\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        \n",
    "        if self.training:\n",
    "            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n",
    "            n = torch.sum(self.ema_count)\n",
    "            self.ema_count = (self.ema_count + self.epsilon) / (n + M * self.epsilon) * n\n",
    "\n",
    "            dw = torch.matmul(encodings.t(), x_flat)\n",
    "            self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw\n",
    "            self.embedding = self.ema_weight / self.ema_count.unsqueeze(-1)\n",
    "\n",
    "        codebook_loss = F.mse_loss(x.detach(), quantized)\n",
    "        e_latent_loss = F.mse_loss(x, quantized.detach())\n",
    "        commitment_loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, commitment_loss, codebook_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_sizes=(1, 3, 2, 2), stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        kernel_1, kernel_2, kernel_3, kernel_4 = kernel_sizes\n",
    "        \n",
    "        self.residual_conv_1 = nn.Conv2d(input_dim, hidden_dim, kernel_1, padding=0)\n",
    "        self.residual_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, padding=1)\n",
    "        \n",
    "        self.strided_t_conv_1 = nn.ConvTranspose2d(hidden_dim, hidden_dim, kernel_3, stride, padding=0)\n",
    "        self.strided_t_conv_2 = nn.ConvTranspose2d(hidden_dim, output_dim, kernel_4, stride, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.residual_conv_1(x)\n",
    "        y = y+x\n",
    "        x = F.relu(y)\n",
    "        \n",
    "        y = self.residual_conv_2(x)\n",
    "        y = y+x\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        y = self.strided_t_conv_1(y)\n",
    "        y = self.strided_t_conv_2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Codebook, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.codebook = Codebook\n",
    "        self.decoder = Decoder\n",
    "                \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_quantized, commitment_loss, codebook_loss, perplexity = self.codebook(z)\n",
    "        x_hat = self.decoder(z_quantized)\n",
    "        \n",
    "        return x_hat, commitment_loss, codebook_loss, perplexity\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=hidden_dim)\n",
    "codebook = VQEmbeddingEMA(n_embeddings=n_embeddings, embedding_dim=hidden_dim)\n",
    "decoder = Decoder(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Codebook=codebook, Decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define Loss function (reprod. loss) and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Train Vector Quantized Variational AutoEncoder (VQ-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VQ-VAE...\n",
      "epoch: 1   step: 1   recon_loss: 0.12986251711845398   perplexity:  47.204471588134766 \n",
      "\t\tcommit_loss:  0.006001153960824013   codebook loss:  0.02400461584329605   total_loss:  0.1598682999610901\n",
      "epoch: 1   step: 51   recon_loss: 0.04567872732877731   perplexity:  42.20817184448242 \n",
      "\t\tcommit_loss:  0.034285638481378555   codebook loss:  0.13714255392551422   total_loss:  0.2171069234609604\n",
      "epoch: 1   step: 101   recon_loss: 0.03384900838136673   perplexity:  89.93128204345703 \n",
      "\t\tcommit_loss:  0.02993432804942131   codebook loss:  0.11973731219768524   total_loss:  0.18352064490318298\n",
      "epoch: 1   step: 151   recon_loss: 0.027672773227095604   perplexity:  124.88264465332031 \n",
      "\t\tcommit_loss:  0.03181006759405136   codebook loss:  0.12724027037620544   total_loss:  0.18672311305999756\n",
      "epoch: 1   step: 201   recon_loss: 0.026118455454707146   perplexity:  154.0612335205078 \n",
      "\t\tcommit_loss:  0.03789319470524788   codebook loss:  0.15157277882099152   total_loss:  0.2155844271183014\n",
      "epoch: 1   step: 251   recon_loss: 0.019762162119150162   perplexity:  181.8699493408203 \n",
      "\t\tcommit_loss:  0.030611857771873474   codebook loss:  0.1224474310874939   total_loss:  0.17282144725322723\n",
      "epoch: 1   step: 301   recon_loss: 0.019178207963705063   perplexity:  254.80259704589844 \n",
      "\t\tcommit_loss:  0.03248064965009689   codebook loss:  0.12992259860038757   total_loss:  0.18158145248889923\n",
      "epoch: 1   step: 351   recon_loss: 0.01647857390344143   perplexity:  332.0083312988281 \n",
      "\t\tcommit_loss:  0.02888965979218483   codebook loss:  0.11555863916873932   total_loss:  0.16092687845230103\n",
      "epoch: 2   step: 1   recon_loss: 0.015931956470012665   perplexity:  378.1283264160156 \n",
      "\t\tcommit_loss:  0.028581345453858376   codebook loss:  0.1143253818154335   total_loss:  0.15883868932724\n",
      "epoch: 2   step: 51   recon_loss: 0.016559787094593048   perplexity:  413.2507629394531 \n",
      "\t\tcommit_loss:  0.030824651941657066   codebook loss:  0.12329860776662827   total_loss:  0.17068305611610413\n",
      "epoch: 2   step: 101   recon_loss: 0.01513717882335186   perplexity:  402.53143310546875 \n",
      "\t\tcommit_loss:  0.028951147571206093   codebook loss:  0.11580459028482437   total_loss:  0.15989291667938232\n",
      "epoch: 2   step: 151   recon_loss: 0.014418624341487885   perplexity:  433.6385192871094 \n",
      "\t\tcommit_loss:  0.028847698122262955   codebook loss:  0.11539079248905182   total_loss:  0.15865711867809296\n",
      "epoch: 2   step: 201   recon_loss: 0.013817387633025646   perplexity:  427.5931701660156 \n",
      "\t\tcommit_loss:  0.028196077793836594   codebook loss:  0.11278431117534637   total_loss:  0.1547977775335312\n",
      "epoch: 2   step: 251   recon_loss: 0.014240259304642677   perplexity:  445.7051086425781 \n",
      "\t\tcommit_loss:  0.030075671151280403   codebook loss:  0.12030268460512161   total_loss:  0.1646186113357544\n",
      "epoch: 2   step: 301   recon_loss: 0.014010995626449585   perplexity:  473.668701171875 \n",
      "\t\tcommit_loss:  0.030258065089583397   codebook loss:  0.12103226035833359   total_loss:  0.16530132293701172\n",
      "epoch: 2   step: 351   recon_loss: 0.01372806541621685   perplexity:  493.0338134765625 \n",
      "\t\tcommit_loss:  0.030602775514125824   codebook loss:  0.1224111020565033   total_loss:  0.16674193739891052\n",
      "epoch: 3   step: 1   recon_loss: 0.012803105637431145   perplexity:  462.90362548828125 \n",
      "\t\tcommit_loss:  0.02747054025530815   codebook loss:  0.1098821610212326   total_loss:  0.15015581250190735\n",
      "epoch: 3   step: 51   recon_loss: 0.012471739202737808   perplexity:  491.0725402832031 \n",
      "\t\tcommit_loss:  0.02764514647424221   codebook loss:  0.11058058589696884   total_loss:  0.1506974697113037\n",
      "epoch: 3   step: 101   recon_loss: 0.012450596317648888   perplexity:  486.9856872558594 \n",
      "\t\tcommit_loss:  0.02803034521639347   codebook loss:  0.11212138086557388   total_loss:  0.15260231494903564\n",
      "epoch: 3   step: 151   recon_loss: 0.012728111818432808   perplexity:  508.5968322753906 \n",
      "\t\tcommit_loss:  0.029185965657234192   codebook loss:  0.11674386262893677   total_loss:  0.15865793824195862\n",
      "epoch: 3   step: 201   recon_loss: 0.01301803719252348   perplexity:  510.0350341796875 \n",
      "\t\tcommit_loss:  0.029723888263106346   codebook loss:  0.11889555305242538   total_loss:  0.16163748502731323\n",
      "epoch: 3   step: 251   recon_loss: 0.012285824865102768   perplexity:  508.84130859375 \n",
      "\t\tcommit_loss:  0.029373876750469208   codebook loss:  0.11749550700187683   total_loss:  0.1591552048921585\n",
      "epoch: 3   step: 301   recon_loss: 0.01262321975082159   perplexity:  535.345458984375 \n",
      "\t\tcommit_loss:  0.028989989310503006   codebook loss:  0.11595995724201202   total_loss:  0.1575731635093689\n",
      "epoch: 3   step: 351   recon_loss: 0.011962058953940868   perplexity:  521.2271728515625 \n",
      "\t\tcommit_loss:  0.02786225639283657   codebook loss:  0.11144902557134628   total_loss:  0.15127333998680115\n",
      "epoch: 4   step: 1   recon_loss: 0.012373367324471474   perplexity:  526.803466796875 \n",
      "\t\tcommit_loss:  0.028743121773004532   codebook loss:  0.11497248709201813   total_loss:  0.15608897805213928\n",
      "epoch: 4   step: 51   recon_loss: 0.01192514505237341   perplexity:  537.25390625 \n",
      "\t\tcommit_loss:  0.02851972170174122   codebook loss:  0.11407888680696487   total_loss:  0.15452376008033752\n",
      "epoch: 4   step: 101   recon_loss: 0.011931141838431358   perplexity:  534.6503295898438 \n",
      "\t\tcommit_loss:  0.027900155633687973   codebook loss:  0.11160062253475189   total_loss:  0.15143191814422607\n",
      "epoch: 4   step: 151   recon_loss: 0.01272584032267332   perplexity:  542.4771728515625 \n",
      "\t\tcommit_loss:  0.02943887561559677   codebook loss:  0.11775550246238708   total_loss:  0.15992021560668945\n",
      "epoch: 4   step: 201   recon_loss: 0.011569542810320854   perplexity:  529.0413818359375 \n",
      "\t\tcommit_loss:  0.027800871059298515   codebook loss:  0.11120348423719406   total_loss:  0.15057389438152313\n",
      "epoch: 4   step: 251   recon_loss: 0.011616121977567673   perplexity:  562.589111328125 \n",
      "\t\tcommit_loss:  0.02912902645766735   codebook loss:  0.1165161058306694   total_loss:  0.15726125240325928\n",
      "epoch: 4   step: 301   recon_loss: 0.01198972761631012   perplexity:  560.057861328125 \n",
      "\t\tcommit_loss:  0.02886773645877838   codebook loss:  0.11547094583511353   total_loss:  0.15632840991020203\n",
      "epoch: 4   step: 351   recon_loss: 0.010793102905154228   perplexity:  536.6494750976562 \n",
      "\t\tcommit_loss:  0.026430338621139526   codebook loss:  0.1057213544845581   total_loss:  0.142944797873497\n",
      "epoch: 5   step: 1   recon_loss: 0.010620544664561749   perplexity:  535.1693725585938 \n",
      "\t\tcommit_loss:  0.026100793853402138   codebook loss:  0.10440317541360855   total_loss:  0.14112451672554016\n",
      "epoch: 5   step: 51   recon_loss: 0.011830231174826622   perplexity:  557.5755004882812 \n",
      "\t\tcommit_loss:  0.029145270586013794   codebook loss:  0.11658108234405518   total_loss:  0.15755659341812134\n",
      "epoch: 5   step: 101   recon_loss: 0.01231419574469328   perplexity:  562.4027099609375 \n",
      "\t\tcommit_loss:  0.030021142214536667   codebook loss:  0.12008456885814667   total_loss:  0.16241991519927979\n",
      "epoch: 5   step: 151   recon_loss: 0.010761063545942307   perplexity:  549.4160766601562 \n",
      "\t\tcommit_loss:  0.027248475700616837   codebook loss:  0.10899390280246735   total_loss:  0.1470034420490265\n",
      "epoch: 5   step: 201   recon_loss: 0.010462814942002296   perplexity:  559.49462890625 \n",
      "\t\tcommit_loss:  0.026431117206811905   codebook loss:  0.10572446882724762   total_loss:  0.14261840283870697\n",
      "epoch: 5   step: 251   recon_loss: 0.010448533110320568   perplexity:  562.6483764648438 \n",
      "\t\tcommit_loss:  0.026403916999697685   codebook loss:  0.10561566799879074   total_loss:  0.14246812462806702\n",
      "epoch: 5   step: 301   recon_loss: 0.011117639020085335   perplexity:  568.7505493164062 \n",
      "\t\tcommit_loss:  0.02767506241798401   codebook loss:  0.11070024967193604   total_loss:  0.14949294924736023\n",
      "epoch: 5   step: 351   recon_loss: 0.011086961254477501   perplexity:  579.7442016601562 \n",
      "\t\tcommit_loss:  0.028620973229408264   codebook loss:  0.11448389291763306   total_loss:  0.15419182181358337\n",
      "epoch: 6   step: 1   recon_loss: 0.011314697563648224   perplexity:  582.0468139648438 \n",
      "\t\tcommit_loss:  0.028508316725492477   codebook loss:  0.11403326690196991   total_loss:  0.1538562774658203\n",
      "epoch: 6   step: 51   recon_loss: 0.010461683385074139   perplexity:  538.056640625 \n",
      "\t\tcommit_loss:  0.02598433755338192   codebook loss:  0.10393735021352768   total_loss:  0.14038336277008057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6   step: 101   recon_loss: 0.010875707492232323   perplexity:  560.1865844726562 \n",
      "\t\tcommit_loss:  0.02711101993918419   codebook loss:  0.10844407975673676   total_loss:  0.14643080532550812\n",
      "epoch: 6   step: 151   recon_loss: 0.01098853349685669   perplexity:  591.0968627929688 \n",
      "\t\tcommit_loss:  0.02788493037223816   codebook loss:  0.11153972148895264   total_loss:  0.15041318535804749\n",
      "epoch: 6   step: 201   recon_loss: 0.010916754603385925   perplexity:  581.0225219726562 \n",
      "\t\tcommit_loss:  0.028284473344683647   codebook loss:  0.11313789337873459   total_loss:  0.1523391306400299\n",
      "epoch: 6   step: 251   recon_loss: 0.010993203148245811   perplexity:  594.9034423828125 \n",
      "\t\tcommit_loss:  0.0285346582531929   codebook loss:  0.1141386330127716   total_loss:  0.15366649627685547\n",
      "epoch: 6   step: 301   recon_loss: 0.010561942122876644   perplexity:  587.4264526367188 \n",
      "\t\tcommit_loss:  0.02696807123720646   codebook loss:  0.10787228494882584   total_loss:  0.14540229737758636\n",
      "epoch: 6   step: 351   recon_loss: 0.010216002352535725   perplexity:  577.9090576171875 \n",
      "\t\tcommit_loss:  0.02720494568347931   codebook loss:  0.10881978273391724   total_loss:  0.1462407261133194\n",
      "epoch: 7   step: 1   recon_loss: 0.010193690657615662   perplexity:  581.3179321289062 \n",
      "\t\tcommit_loss:  0.027027692645788193   codebook loss:  0.10811077058315277   total_loss:  0.14533215761184692\n",
      "epoch: 7   step: 51   recon_loss: 0.01083354838192463   perplexity:  605.9788208007812 \n",
      "\t\tcommit_loss:  0.027452945709228516   codebook loss:  0.10981178283691406   total_loss:  0.14809827506542206\n",
      "epoch: 7   step: 101   recon_loss: 0.010060637257993221   perplexity:  591.3759765625 \n",
      "\t\tcommit_loss:  0.027286788448691368   codebook loss:  0.10914715379476547   total_loss:  0.14649458229541779\n",
      "epoch: 7   step: 151   recon_loss: 0.010642136447131634   perplexity:  599.5195922851562 \n",
      "\t\tcommit_loss:  0.028104519471526146   codebook loss:  0.11241807788610458   total_loss:  0.15116474032402039\n",
      "epoch: 7   step: 201   recon_loss: 0.009940480813384056   perplexity:  576.7332763671875 \n",
      "\t\tcommit_loss:  0.02596288174390793   codebook loss:  0.10385152697563171   total_loss:  0.13975489139556885\n",
      "epoch: 7   step: 251   recon_loss: 0.009521452710032463   perplexity:  578.1618041992188 \n",
      "\t\tcommit_loss:  0.025239968672394753   codebook loss:  0.10095987468957901   total_loss:  0.13572129607200623\n",
      "epoch: 7   step: 301   recon_loss: 0.011413948610424995   perplexity:  621.3030395507812 \n",
      "\t\tcommit_loss:  0.02988109365105629   codebook loss:  0.11952437460422516   total_loss:  0.160819411277771\n",
      "epoch: 7   step: 351   recon_loss: 0.010653296485543251   perplexity:  615.8983154296875 \n",
      "\t\tcommit_loss:  0.028028832748532295   codebook loss:  0.11211533099412918   total_loss:  0.15079745650291443\n",
      "epoch: 8   step: 1   recon_loss: 0.010207246989011765   perplexity:  595.64453125 \n",
      "\t\tcommit_loss:  0.02780880033969879   codebook loss:  0.11123520135879517   total_loss:  0.14925125241279602\n",
      "epoch: 8   step: 51   recon_loss: 0.009696176275610924   perplexity:  582.47802734375 \n",
      "\t\tcommit_loss:  0.025791101157665253   codebook loss:  0.10316440463066101   total_loss:  0.13865168392658234\n",
      "epoch: 8   step: 101   recon_loss: 0.010204214602708817   perplexity:  607.0494995117188 \n",
      "\t\tcommit_loss:  0.026853322982788086   codebook loss:  0.10741329193115234   total_loss:  0.14447082579135895\n",
      "epoch: 8   step: 151   recon_loss: 0.010414741933345795   perplexity:  614.4190063476562 \n",
      "\t\tcommit_loss:  0.02794797345995903   codebook loss:  0.11179189383983612   total_loss:  0.15015460550785065\n",
      "epoch: 8   step: 201   recon_loss: 0.00989119615405798   perplexity:  592.2044677734375 \n",
      "\t\tcommit_loss:  0.026432782411575317   codebook loss:  0.10573112964630127   total_loss:  0.14205510914325714\n",
      "epoch: 8   step: 251   recon_loss: 0.009289266541600227   perplexity:  611.4866333007812 \n",
      "\t\tcommit_loss:  0.025730442255735397   codebook loss:  0.10292176902294159   total_loss:  0.13794147968292236\n",
      "epoch: 8   step: 301   recon_loss: 0.009568857029080391   perplexity:  594.4474487304688 \n",
      "\t\tcommit_loss:  0.026959184557199478   codebook loss:  0.10783673822879791   total_loss:  0.14436477422714233\n",
      "epoch: 8   step: 351   recon_loss: 0.010323853231966496   perplexity:  620.098388671875 \n",
      "\t\tcommit_loss:  0.027910422533750534   codebook loss:  0.11164169013500214   total_loss:  0.1498759686946869\n",
      "epoch: 9   step: 1   recon_loss: 0.009754414670169353   perplexity:  613.048583984375 \n",
      "\t\tcommit_loss:  0.026248089969158173   codebook loss:  0.10499235987663269   total_loss:  0.1409948617219925\n",
      "epoch: 9   step: 51   recon_loss: 0.010098153725266457   perplexity:  616.2108764648438 \n",
      "\t\tcommit_loss:  0.027812551707029343   codebook loss:  0.11125020682811737   total_loss:  0.14916092157363892\n",
      "epoch: 9   step: 101   recon_loss: 0.010359751991927624   perplexity:  632.4307250976562 \n",
      "\t\tcommit_loss:  0.028120163828134537   codebook loss:  0.11248065531253815   total_loss:  0.15096056461334229\n",
      "epoch: 9   step: 151   recon_loss: 0.00898471288383007   perplexity:  593.283935546875 \n",
      "\t\tcommit_loss:  0.02492763102054596   codebook loss:  0.09971052408218384   total_loss:  0.13362286984920502\n",
      "epoch: 9   step: 201   recon_loss: 0.009680764749646187   perplexity:  614.3115234375 \n",
      "\t\tcommit_loss:  0.027485547587275505   codebook loss:  0.10994219034910202   total_loss:  0.14710849523544312\n",
      "epoch: 9   step: 251   recon_loss: 0.009200693108141422   perplexity:  602.7282104492188 \n",
      "\t\tcommit_loss:  0.025777151808142662   codebook loss:  0.10310860723257065   total_loss:  0.1380864530801773\n",
      "epoch: 9   step: 301   recon_loss: 0.009717186912894249   perplexity:  619.0965576171875 \n",
      "\t\tcommit_loss:  0.02750985510647297   codebook loss:  0.11003942042589188   total_loss:  0.1472664624452591\n",
      "epoch: 9   step: 351   recon_loss: 0.010195085778832436   perplexity:  616.123046875 \n",
      "\t\tcommit_loss:  0.027368497103452682   codebook loss:  0.10947398841381073   total_loss:  0.1470375657081604\n",
      "epoch: 10   step: 1   recon_loss: 0.008629092946648598   perplexity:  611.498291015625 \n",
      "\t\tcommit_loss:  0.023804206401109695   codebook loss:  0.09521682560443878   total_loss:  0.12765012681484222\n",
      "epoch: 10   step: 51   recon_loss: 0.00955715961754322   perplexity:  614.7197265625 \n",
      "\t\tcommit_loss:  0.026747487485408783   codebook loss:  0.10698994994163513   total_loss:  0.14329460263252258\n",
      "epoch: 10   step: 101   recon_loss: 0.01069907657802105   perplexity:  639.7662963867188 \n",
      "\t\tcommit_loss:  0.0294661782681942   codebook loss:  0.1178647130727768   total_loss:  0.1580299735069275\n",
      "epoch: 10   step: 151   recon_loss: 0.01008719764649868   perplexity:  638.18408203125 \n",
      "\t\tcommit_loss:  0.02787349373102188   codebook loss:  0.11149397492408752   total_loss:  0.14945466816425323\n",
      "epoch: 10   step: 201   recon_loss: 0.009747960604727268   perplexity:  630.7736206054688 \n",
      "\t\tcommit_loss:  0.02810100093483925   codebook loss:  0.112404003739357   total_loss:  0.15025296807289124\n",
      "epoch: 10   step: 251   recon_loss: 0.00974769052118063   perplexity:  625.5618286132812 \n",
      "\t\tcommit_loss:  0.02756207063794136   codebook loss:  0.11024828255176544   total_loss:  0.1475580483675003\n",
      "epoch: 10   step: 301   recon_loss: 0.009658342227339745   perplexity:  622.6070556640625 \n",
      "\t\tcommit_loss:  0.02735309675335884   codebook loss:  0.10941238701343536   total_loss:  0.1464238166809082\n",
      "epoch: 10   step: 351   recon_loss: 0.00985830556601286   perplexity:  631.5790405273438 \n",
      "\t\tcommit_loss:  0.029222358018159866   codebook loss:  0.11688943207263947   total_loss:  0.15597009658813477\n",
      "epoch: 11   step: 1   recon_loss: 0.009156222455203533   perplexity:  614.5368041992188 \n",
      "\t\tcommit_loss:  0.026225149631500244   codebook loss:  0.10490059852600098   total_loss:  0.14028197526931763\n",
      "epoch: 11   step: 51   recon_loss: 0.00978030078113079   perplexity:  629.0277099609375 \n",
      "\t\tcommit_loss:  0.02792458049952984   codebook loss:  0.11169832199811935   total_loss:  0.14940319955348969\n",
      "epoch: 11   step: 101   recon_loss: 0.0095300218090415   perplexity:  639.1403198242188 \n",
      "\t\tcommit_loss:  0.027853481471538544   codebook loss:  0.11141392588615417   total_loss:  0.1487974226474762\n",
      "epoch: 11   step: 151   recon_loss: 0.009797035716474056   perplexity:  639.041259765625 \n",
      "\t\tcommit_loss:  0.0276726633310318   codebook loss:  0.1106906533241272   total_loss:  0.14816035330295563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11   step: 201   recon_loss: 0.009423065930604935   perplexity:  622.7198486328125 \n",
      "\t\tcommit_loss:  0.027028506621718407   codebook loss:  0.10811402648687363   total_loss:  0.14456559717655182\n",
      "epoch: 11   step: 251   recon_loss: 0.009687854908406734   perplexity:  622.070556640625 \n",
      "\t\tcommit_loss:  0.02676006406545639   codebook loss:  0.10704025626182556   total_loss:  0.14348816871643066\n",
      "epoch: 11   step: 301   recon_loss: 0.009648793376982212   perplexity:  630.8403930664062 \n",
      "\t\tcommit_loss:  0.0264489334076643   codebook loss:  0.1057957336306572   total_loss:  0.14189346134662628\n",
      "epoch: 11   step: 351   recon_loss: 0.009450634010136127   perplexity:  638.7750244140625 \n",
      "\t\tcommit_loss:  0.027651259675621986   codebook loss:  0.11060503870248795   total_loss:  0.14770692586898804\n",
      "epoch: 12   step: 1   recon_loss: 0.009017271921038628   perplexity:  605.3821411132812 \n",
      "\t\tcommit_loss:  0.02586526609957218   codebook loss:  0.10346106439828873   total_loss:  0.13834360241889954\n",
      "epoch: 12   step: 51   recon_loss: 0.00993420835584402   perplexity:  636.4976806640625 \n",
      "\t\tcommit_loss:  0.0280138049274683   codebook loss:  0.1120552197098732   total_loss:  0.15000322461128235\n",
      "epoch: 12   step: 101   recon_loss: 0.009532256983220577   perplexity:  638.6802978515625 \n",
      "\t\tcommit_loss:  0.028101032599806786   codebook loss:  0.11240413039922714   total_loss:  0.15003742277622223\n",
      "epoch: 12   step: 151   recon_loss: 0.009122883901000023   perplexity:  642.7681274414062 \n",
      "\t\tcommit_loss:  0.027097417041659355   codebook loss:  0.10838966816663742   total_loss:  0.1446099728345871\n",
      "epoch: 12   step: 201   recon_loss: 0.009248270653188229   perplexity:  628.270751953125 \n",
      "\t\tcommit_loss:  0.026473965495824814   codebook loss:  0.10589586198329926   total_loss:  0.14161810278892517\n",
      "epoch: 12   step: 251   recon_loss: 0.00967535562813282   perplexity:  629.7044067382812 \n",
      "\t\tcommit_loss:  0.02694633975625038   codebook loss:  0.10778535902500153   total_loss:  0.14440706372261047\n",
      "epoch: 12   step: 301   recon_loss: 0.009529337286949158   perplexity:  645.4186401367188 \n",
      "\t\tcommit_loss:  0.028523778542876244   codebook loss:  0.11409511417150497   total_loss:  0.15214823186397552\n",
      "epoch: 12   step: 351   recon_loss: 0.009559313766658306   perplexity:  628.4535522460938 \n",
      "\t\tcommit_loss:  0.02809467725455761   codebook loss:  0.11237870901823044   total_loss:  0.15003269910812378\n",
      "epoch: 13   step: 1   recon_loss: 0.008273832499980927   perplexity:  609.0382080078125 \n",
      "\t\tcommit_loss:  0.02371455729007721   codebook loss:  0.09485822916030884   total_loss:  0.12684661149978638\n",
      "epoch: 13   step: 51   recon_loss: 0.00919179618358612   perplexity:  643.7932739257812 \n",
      "\t\tcommit_loss:  0.02728220820426941   codebook loss:  0.10912883281707764   total_loss:  0.14560283720493317\n",
      "epoch: 13   step: 101   recon_loss: 0.009501026943325996   perplexity:  643.9464721679688 \n",
      "\t\tcommit_loss:  0.02720346674323082   codebook loss:  0.10881386697292328   total_loss:  0.14551836252212524\n",
      "epoch: 13   step: 151   recon_loss: 0.009045179933309555   perplexity:  635.1597290039062 \n",
      "\t\tcommit_loss:  0.026926781982183456   codebook loss:  0.10770712792873383   total_loss:  0.14367908239364624\n",
      "epoch: 13   step: 201   recon_loss: 0.009018110111355782   perplexity:  621.5317993164062 \n",
      "\t\tcommit_loss:  0.02643836848437786   codebook loss:  0.10575347393751144   total_loss:  0.14120995998382568\n",
      "epoch: 13   step: 251   recon_loss: 0.008431238122284412   perplexity:  616.2158813476562 \n",
      "\t\tcommit_loss:  0.024730397388339043   codebook loss:  0.09892158955335617   total_loss:  0.1320832222700119\n",
      "epoch: 13   step: 301   recon_loss: 0.009020866826176643   perplexity:  632.7260131835938 \n",
      "\t\tcommit_loss:  0.027124017477035522   codebook loss:  0.10849606990814209   total_loss:  0.1446409523487091\n",
      "epoch: 13   step: 351   recon_loss: 0.009493987075984478   perplexity:  636.1102294921875 \n",
      "\t\tcommit_loss:  0.026918664574623108   codebook loss:  0.10767465829849243   total_loss:  0.1440873146057129\n",
      "epoch: 14   step: 1   recon_loss: 0.009195094928145409   perplexity:  642.100341796875 \n",
      "\t\tcommit_loss:  0.027640536427497864   codebook loss:  0.11056214570999146   total_loss:  0.14739778637886047\n",
      "epoch: 14   step: 51   recon_loss: 0.009370232000946999   perplexity:  634.0618896484375 \n",
      "\t\tcommit_loss:  0.028587166219949722   codebook loss:  0.11434866487979889   total_loss:  0.15230606496334076\n",
      "epoch: 14   step: 101   recon_loss: 0.009170192293822765   perplexity:  636.0298461914062 \n",
      "\t\tcommit_loss:  0.02696838416159153   codebook loss:  0.10787353664636612   total_loss:  0.14401210844516754\n",
      "epoch: 14   step: 151   recon_loss: 0.009315943345427513   perplexity:  644.3380737304688 \n",
      "\t\tcommit_loss:  0.027763893827795982   codebook loss:  0.11105557531118393   total_loss:  0.14813540875911713\n",
      "epoch: 14   step: 201   recon_loss: 0.008920082822442055   perplexity:  626.2841186523438 \n",
      "\t\tcommit_loss:  0.027287516742944717   codebook loss:  0.10915006697177887   total_loss:  0.1453576683998108\n",
      "epoch: 14   step: 251   recon_loss: 0.009117420762777328   perplexity:  642.8101806640625 \n",
      "\t\tcommit_loss:  0.02709009498357773   codebook loss:  0.10836037993431091   total_loss:  0.14456789195537567\n",
      "epoch: 14   step: 301   recon_loss: 0.008915255777537823   perplexity:  626.6300048828125 \n",
      "\t\tcommit_loss:  0.026486540213227272   codebook loss:  0.10594616085290909   total_loss:  0.1413479596376419\n",
      "epoch: 14   step: 351   recon_loss: 0.008728188462555408   perplexity:  634.2245483398438 \n",
      "\t\tcommit_loss:  0.026503685861825943   codebook loss:  0.10601474344730377   total_loss:  0.14124661684036255\n",
      "epoch: 15   step: 1   recon_loss: 0.009091861546039581   perplexity:  643.2703857421875 \n",
      "\t\tcommit_loss:  0.027176175266504288   codebook loss:  0.10870470106601715   total_loss:  0.14497274160385132\n",
      "epoch: 15   step: 51   recon_loss: 0.009447379037737846   perplexity:  647.9268188476562 \n",
      "\t\tcommit_loss:  0.028961362317204475   codebook loss:  0.1158454492688179   total_loss:  0.15425419807434082\n",
      "epoch: 15   step: 101   recon_loss: 0.009345145896077156   perplexity:  655.1162719726562 \n",
      "\t\tcommit_loss:  0.027330543845891953   codebook loss:  0.10932217538356781   total_loss:  0.14599786698818207\n",
      "epoch: 15   step: 151   recon_loss: 0.008310086093842983   perplexity:  622.9150390625 \n",
      "\t\tcommit_loss:  0.02532370761036873   codebook loss:  0.10129483044147491   total_loss:  0.1349286288022995\n",
      "epoch: 15   step: 201   recon_loss: 0.008827421814203262   perplexity:  625.6683349609375 \n",
      "\t\tcommit_loss:  0.026955103501677513   codebook loss:  0.10782041400671005   total_loss:  0.14360293745994568\n",
      "epoch: 15   step: 251   recon_loss: 0.008668972179293633   perplexity:  613.761962890625 \n",
      "\t\tcommit_loss:  0.02556062489748001   codebook loss:  0.10224249958992004   total_loss:  0.13647210597991943\n",
      "epoch: 15   step: 301   recon_loss: 0.008867183700203896   perplexity:  629.335205078125 \n",
      "\t\tcommit_loss:  0.026440873742103577   codebook loss:  0.1057634949684143   total_loss:  0.14107155799865723\n",
      "epoch: 15   step: 351   recon_loss: 0.008718874305486679   perplexity:  636.88232421875 \n",
      "\t\tcommit_loss:  0.02692520245909691   codebook loss:  0.10770080983638763   total_loss:  0.14334487915039062\n",
      "epoch: 16   step: 1   recon_loss: 0.00872789416462183   perplexity:  640.2542724609375 \n",
      "\t\tcommit_loss:  0.026675177738070488   codebook loss:  0.10670071095228195   total_loss:  0.14210379123687744\n",
      "epoch: 16   step: 51   recon_loss: 0.008660800755023956   perplexity:  635.3523559570312 \n",
      "\t\tcommit_loss:  0.026650553569197655   codebook loss:  0.10660221427679062   total_loss:  0.14191356301307678\n",
      "epoch: 16   step: 101   recon_loss: 0.008452443405985832   perplexity:  632.75830078125 \n",
      "\t\tcommit_loss:  0.026264743879437447   codebook loss:  0.10505897551774979   total_loss:  0.13977617025375366\n",
      "epoch: 16   step: 151   recon_loss: 0.009113401174545288   perplexity:  640.116943359375 \n",
      "\t\tcommit_loss:  0.02741757035255432   codebook loss:  0.10967028141021729   total_loss:  0.1462012529373169\n",
      "epoch: 16   step: 201   recon_loss: 0.009076950140297413   perplexity:  643.7279052734375 \n",
      "\t\tcommit_loss:  0.028261305764317513   codebook loss:  0.11304522305727005   total_loss:  0.15038347244262695\n",
      "epoch: 16   step: 251   recon_loss: 0.008821230381727219   perplexity:  624.8856201171875 \n",
      "\t\tcommit_loss:  0.026189301162958145   codebook loss:  0.10475720465183258   total_loss:  0.13976773619651794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16   step: 301   recon_loss: 0.00853740330785513   perplexity:  626.6061401367188 \n",
      "\t\tcommit_loss:  0.02685229852795601   codebook loss:  0.10740919411182404   total_loss:  0.14279890060424805\n",
      "epoch: 16   step: 351   recon_loss: 0.00862729363143444   perplexity:  635.871826171875 \n",
      "\t\tcommit_loss:  0.026677733287215233   codebook loss:  0.10671093314886093   total_loss:  0.1420159637928009\n",
      "epoch: 17   step: 1   recon_loss: 0.008372170850634575   perplexity:  621.5608520507812 \n",
      "\t\tcommit_loss:  0.025481699034571648   codebook loss:  0.10192679613828659   total_loss:  0.13578066229820251\n",
      "epoch: 17   step: 51   recon_loss: 0.008781087584793568   perplexity:  648.1851806640625 \n",
      "\t\tcommit_loss:  0.028656475245952606   codebook loss:  0.11462590098381042   total_loss:  0.15206345915794373\n",
      "epoch: 17   step: 101   recon_loss: 0.009105941280722618   perplexity:  647.7300415039062 \n",
      "\t\tcommit_loss:  0.02814292535185814   codebook loss:  0.11257170140743256   total_loss:  0.14982056617736816\n",
      "epoch: 17   step: 151   recon_loss: 0.009031913243234158   perplexity:  642.1420288085938 \n",
      "\t\tcommit_loss:  0.028461694717407227   codebook loss:  0.1138467788696289   total_loss:  0.15134039521217346\n",
      "epoch: 17   step: 201   recon_loss: 0.009112492203712463   perplexity:  649.230712890625 \n",
      "\t\tcommit_loss:  0.02791725844144821   codebook loss:  0.11166903376579285   total_loss:  0.14869877696037292\n",
      "epoch: 17   step: 251   recon_loss: 0.00870419479906559   perplexity:  641.510009765625 \n",
      "\t\tcommit_loss:  0.02678050845861435   codebook loss:  0.1071220338344574   total_loss:  0.1426067352294922\n",
      "epoch: 17   step: 301   recon_loss: 0.009156920947134495   perplexity:  646.0350952148438 \n",
      "\t\tcommit_loss:  0.02732163295149803   codebook loss:  0.10928653180599213   total_loss:  0.14576508104801178\n",
      "epoch: 17   step: 351   recon_loss: 0.00896674394607544   perplexity:  640.151123046875 \n",
      "\t\tcommit_loss:  0.02741914987564087   codebook loss:  0.10967659950256348   total_loss:  0.14606249332427979\n",
      "epoch: 18   step: 1   recon_loss: 0.009017555974423885   perplexity:  646.127197265625 \n",
      "\t\tcommit_loss:  0.027587540447711945   codebook loss:  0.11035016179084778   total_loss:  0.14695525169372559\n",
      "epoch: 18   step: 51   recon_loss: 0.008338573388755322   perplexity:  636.8519287109375 \n",
      "\t\tcommit_loss:  0.026496272534132004   codebook loss:  0.10598509013652802   total_loss:  0.14081993699073792\n",
      "epoch: 18   step: 101   recon_loss: 0.008596302941441536   perplexity:  637.6107177734375 \n",
      "\t\tcommit_loss:  0.026641974225640297   codebook loss:  0.10656789690256119   total_loss:  0.14180617034435272\n",
      "epoch: 18   step: 151   recon_loss: 0.00826007779687643   perplexity:  619.9801635742188 \n",
      "\t\tcommit_loss:  0.02497630938887596   codebook loss:  0.09990523755550385   total_loss:  0.1331416219472885\n",
      "epoch: 18   step: 201   recon_loss: 0.009006845764815807   perplexity:  660.470458984375 \n",
      "\t\tcommit_loss:  0.02862992137670517   codebook loss:  0.11451968550682068   total_loss:  0.15215645730495453\n",
      "epoch: 18   step: 251   recon_loss: 0.008722515776753426   perplexity:  644.8864135742188 \n",
      "\t\tcommit_loss:  0.0281108058989048   codebook loss:  0.1124432235956192   total_loss:  0.14927655458450317\n",
      "epoch: 18   step: 301   recon_loss: 0.008252776227891445   perplexity:  640.1205444335938 \n",
      "\t\tcommit_loss:  0.026682812720537186   codebook loss:  0.10673125088214874   total_loss:  0.14166684448719025\n",
      "epoch: 18   step: 351   recon_loss: 0.007808453869074583   perplexity:  619.4913940429688 \n",
      "\t\tcommit_loss:  0.0247175432741642   codebook loss:  0.0988701730966568   total_loss:  0.13139617443084717\n",
      "epoch: 19   step: 1   recon_loss: 0.008035906590521336   perplexity:  626.7644653320312 \n",
      "\t\tcommit_loss:  0.025741074234247208   codebook loss:  0.10296429693698883   total_loss:  0.1367412805557251\n",
      "epoch: 19   step: 51   recon_loss: 0.008992640301585197   perplexity:  653.3695678710938 \n",
      "\t\tcommit_loss:  0.027860447764396667   codebook loss:  0.11144179105758667   total_loss:  0.14829488098621368\n",
      "epoch: 19   step: 101   recon_loss: 0.008678622543811798   perplexity:  629.5272827148438 \n",
      "\t\tcommit_loss:  0.027478273957967758   codebook loss:  0.10991309583187103   total_loss:  0.1460699886083603\n",
      "epoch: 19   step: 151   recon_loss: 0.009190080687403679   perplexity:  647.2530517578125 \n",
      "\t\tcommit_loss:  0.029083050787448883   codebook loss:  0.11633220314979553   total_loss:  0.15460532903671265\n",
      "epoch: 19   step: 201   recon_loss: 0.008313183672726154   perplexity:  635.5941772460938 \n",
      "\t\tcommit_loss:  0.026544297114014626   codebook loss:  0.1061771884560585   total_loss:  0.14103466272354126\n",
      "epoch: 19   step: 251   recon_loss: 0.008180120959877968   perplexity:  628.1521606445312 \n",
      "\t\tcommit_loss:  0.02586507424712181   codebook loss:  0.10346029698848724   total_loss:  0.13750550150871277\n",
      "epoch: 19   step: 301   recon_loss: 0.008496777154505253   perplexity:  651.4578857421875 \n",
      "\t\tcommit_loss:  0.027166109532117844   codebook loss:  0.10866443812847137   total_loss:  0.1443273276090622\n",
      "epoch: 19   step: 351   recon_loss: 0.007855063304305077   perplexity:  633.4366455078125 \n",
      "\t\tcommit_loss:  0.024852056056261063   codebook loss:  0.09940822422504425   total_loss:  0.13211533427238464\n",
      "epoch: 20   step: 1   recon_loss: 0.008789269253611565   perplexity:  640.95654296875 \n",
      "\t\tcommit_loss:  0.02908918634057045   codebook loss:  0.1163567453622818   total_loss:  0.15423519909381866\n",
      "epoch: 20   step: 51   recon_loss: 0.008910463191568851   perplexity:  654.1617431640625 \n",
      "\t\tcommit_loss:  0.02773212268948555   codebook loss:  0.1109284907579422   total_loss:  0.14757107198238373\n",
      "epoch: 20   step: 101   recon_loss: 0.008387673646211624   perplexity:  641.5255737304688 \n",
      "\t\tcommit_loss:  0.026988841593265533   codebook loss:  0.10795536637306213   total_loss:  0.1433318853378296\n",
      "epoch: 20   step: 151   recon_loss: 0.008283017203211784   perplexity:  639.3176879882812 \n",
      "\t\tcommit_loss:  0.027124259620904922   codebook loss:  0.10849703848361969   total_loss:  0.14390431344509125\n",
      "epoch: 20   step: 201   recon_loss: 0.009482401423156261   perplexity:  657.940673828125 \n",
      "\t\tcommit_loss:  0.0300617553293705   codebook loss:  0.120247021317482   total_loss:  0.15979117155075073\n",
      "epoch: 20   step: 251   recon_loss: 0.008187294006347656   perplexity:  618.714111328125 \n",
      "\t\tcommit_loss:  0.02532338723540306   codebook loss:  0.10129354894161224   total_loss:  0.13480423390865326\n",
      "epoch: 20   step: 301   recon_loss: 0.008236035704612732   perplexity:  636.8713989257812 \n",
      "\t\tcommit_loss:  0.026577897369861603   codebook loss:  0.10631158947944641   total_loss:  0.14112553000450134\n",
      "epoch: 20   step: 351   recon_loss: 0.008071979507803917   perplexity:  628.48828125 \n",
      "\t\tcommit_loss:  0.026025790721178055   codebook loss:  0.10410316288471222   total_loss:  0.13820093870162964\n",
      "epoch: 21   step: 1   recon_loss: 0.008124986663460732   perplexity:  623.7853393554688 \n",
      "\t\tcommit_loss:  0.02540583536028862   codebook loss:  0.10162334144115448   total_loss:  0.13515415787696838\n",
      "epoch: 21   step: 51   recon_loss: 0.008118653669953346   perplexity:  644.79541015625 \n",
      "\t\tcommit_loss:  0.025606229901313782   codebook loss:  0.10242491960525513   total_loss:  0.1361497938632965\n",
      "epoch: 21   step: 101   recon_loss: 0.008523707278072834   perplexity:  646.919189453125 \n",
      "\t\tcommit_loss:  0.027759507298469543   codebook loss:  0.11103802919387817   total_loss:  0.14732123911380768\n",
      "epoch: 21   step: 151   recon_loss: 0.007876653224229813   perplexity:  632.5802612304688 \n",
      "\t\tcommit_loss:  0.025802455842494965   codebook loss:  0.10320982336997986   total_loss:  0.13688893616199493\n",
      "epoch: 21   step: 201   recon_loss: 0.008663713000714779   perplexity:  632.703369140625 \n",
      "\t\tcommit_loss:  0.027251344174146652   codebook loss:  0.10900537669658661   total_loss:  0.1449204385280609\n",
      "epoch: 21   step: 251   recon_loss: 0.008921472355723381   perplexity:  665.9766235351562 \n",
      "\t\tcommit_loss:  0.027526458725333214   codebook loss:  0.11010583490133286   total_loss:  0.14655376970767975\n",
      "epoch: 21   step: 301   recon_loss: 0.008419170044362545   perplexity:  644.2059936523438 \n",
      "\t\tcommit_loss:  0.027219131588935852   codebook loss:  0.10887652635574341   total_loss:  0.14451482892036438\n",
      "epoch: 21   step: 351   recon_loss: 0.008399292826652527   perplexity:  639.3155517578125 \n",
      "\t\tcommit_loss:  0.026272550225257874   codebook loss:  0.1050902009010315   total_loss:  0.1397620439529419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22   step: 1   recon_loss: 0.008469371125102043   perplexity:  644.9039306640625 \n",
      "\t\tcommit_loss:  0.02729184553027153   codebook loss:  0.10916738212108612   total_loss:  0.14492860436439514\n",
      "epoch: 22   step: 51   recon_loss: 0.007823950611054897   perplexity:  632.595947265625 \n",
      "\t\tcommit_loss:  0.02530590258538723   codebook loss:  0.10122361034154892   total_loss:  0.13435345888137817\n",
      "epoch: 22   step: 101   recon_loss: 0.007742434274405241   perplexity:  635.242431640625 \n",
      "\t\tcommit_loss:  0.025445587933063507   codebook loss:  0.10178235173225403   total_loss:  0.13497036695480347\n",
      "epoch: 22   step: 151   recon_loss: 0.008194840513169765   perplexity:  639.1213989257812 \n",
      "\t\tcommit_loss:  0.02676723524928093   codebook loss:  0.10706894099712372   total_loss:  0.1420310139656067\n",
      "epoch: 22   step: 201   recon_loss: 0.00791831873357296   perplexity:  632.6032104492188 \n",
      "\t\tcommit_loss:  0.025595862418413162   codebook loss:  0.10238344967365265   total_loss:  0.13589763641357422\n",
      "epoch: 22   step: 251   recon_loss: 0.008064251393079758   perplexity:  652.9494018554688 \n",
      "\t\tcommit_loss:  0.025585340335965157   codebook loss:  0.10234136134386063   total_loss:  0.1359909474849701\n",
      "epoch: 22   step: 301   recon_loss: 0.008971203118562698   perplexity:  656.077880859375 \n",
      "\t\tcommit_loss:  0.028801605105400085   codebook loss:  0.11520642042160034   total_loss:  0.15297922492027283\n",
      "epoch: 22   step: 351   recon_loss: 0.007871029898524284   perplexity:  637.0408325195312 \n",
      "\t\tcommit_loss:  0.025929605588316917   codebook loss:  0.10371842235326767   total_loss:  0.13751906156539917\n",
      "epoch: 23   step: 1   recon_loss: 0.00800081342458725   perplexity:  631.1443481445312 \n",
      "\t\tcommit_loss:  0.026317071169614792   codebook loss:  0.10526828467845917   total_loss:  0.1395861655473709\n",
      "epoch: 23   step: 51   recon_loss: 0.008011204190552235   perplexity:  637.1471557617188 \n",
      "\t\tcommit_loss:  0.025941815227270126   codebook loss:  0.1037672609090805   total_loss:  0.1377202868461609\n",
      "epoch: 23   step: 101   recon_loss: 0.007757009472697973   perplexity:  643.486328125 \n",
      "\t\tcommit_loss:  0.025357786566019058   codebook loss:  0.10143114626407623   total_loss:  0.1345459371805191\n",
      "epoch: 23   step: 151   recon_loss: 0.008008606731891632   perplexity:  651.122802734375 \n",
      "\t\tcommit_loss:  0.02627386525273323   codebook loss:  0.10509546101093292   total_loss:  0.13937793672084808\n",
      "epoch: 23   step: 201   recon_loss: 0.007948718033730984   perplexity:  645.0479125976562 \n",
      "\t\tcommit_loss:  0.025652503594756126   codebook loss:  0.1026100143790245   total_loss:  0.13621123135089874\n",
      "epoch: 23   step: 251   recon_loss: 0.008464435115456581   perplexity:  661.7566528320312 \n",
      "\t\tcommit_loss:  0.02710380032658577   codebook loss:  0.10841520130634308   total_loss:  0.14398343861103058\n",
      "epoch: 23   step: 301   recon_loss: 0.008927681483328342   perplexity:  655.4618530273438 \n",
      "\t\tcommit_loss:  0.028461052104830742   codebook loss:  0.11384420841932297   total_loss:  0.15123294293880463\n",
      "epoch: 23   step: 351   recon_loss: 0.008299007080495358   perplexity:  655.18408203125 \n",
      "\t\tcommit_loss:  0.027096424251794815   codebook loss:  0.10838569700717926   total_loss:  0.1437811255455017\n",
      "epoch: 24   step: 1   recon_loss: 0.008741242811083794   perplexity:  656.3532104492188 \n",
      "\t\tcommit_loss:  0.029070915654301643   codebook loss:  0.11628366261720657   total_loss:  0.1540958285331726\n",
      "epoch: 24   step: 51   recon_loss: 0.007784063927829266   perplexity:  624.9649047851562 \n",
      "\t\tcommit_loss:  0.024488117545843124   codebook loss:  0.0979524701833725   total_loss:  0.13022464513778687\n",
      "epoch: 24   step: 101   recon_loss: 0.008392231538891792   perplexity:  647.3805541992188 \n",
      "\t\tcommit_loss:  0.027081865817308426   codebook loss:  0.1083274632692337   total_loss:  0.14380156993865967\n",
      "epoch: 24   step: 151   recon_loss: 0.007998848333954811   perplexity:  642.281005859375 \n",
      "\t\tcommit_loss:  0.02621587924659252   codebook loss:  0.10486351698637009   total_loss:  0.13907824456691742\n",
      "epoch: 24   step: 201   recon_loss: 0.00844993069767952   perplexity:  659.8408813476562 \n",
      "\t\tcommit_loss:  0.027760513126850128   codebook loss:  0.11104205250740051   total_loss:  0.14725250005722046\n",
      "epoch: 24   step: 251   recon_loss: 0.007950220257043839   perplexity:  636.7845458984375 \n",
      "\t\tcommit_loss:  0.026289653033018112   codebook loss:  0.10515861213207245   total_loss:  0.1393984854221344\n",
      "epoch: 24   step: 301   recon_loss: 0.008380269631743431   perplexity:  650.6001586914062 \n",
      "\t\tcommit_loss:  0.02706282027065754   codebook loss:  0.10825128108263016   total_loss:  0.14369437098503113\n",
      "epoch: 24   step: 351   recon_loss: 0.008394476026296616   perplexity:  637.614013671875 \n",
      "\t\tcommit_loss:  0.02648283541202545   codebook loss:  0.1059313416481018   total_loss:  0.14080865681171417\n",
      "epoch: 25   step: 1   recon_loss: 0.007519626524299383   perplexity:  623.4790649414062 \n",
      "\t\tcommit_loss:  0.02501620352268219   codebook loss:  0.10006481409072876   total_loss:  0.13260063529014587\n",
      "epoch: 25   step: 51   recon_loss: 0.00820888765156269   perplexity:  642.0844116210938 \n",
      "\t\tcommit_loss:  0.025685295462608337   codebook loss:  0.10274118185043335   total_loss:  0.13663536310195923\n",
      "epoch: 25   step: 101   recon_loss: 0.007610092870891094   perplexity:  615.5131225585938 \n",
      "\t\tcommit_loss:  0.024961192160844803   codebook loss:  0.09984476864337921   total_loss:  0.13241605460643768\n",
      "epoch: 25   step: 151   recon_loss: 0.00830586813390255   perplexity:  648.4739379882812 \n",
      "\t\tcommit_loss:  0.02701478824019432   codebook loss:  0.10805915296077728   total_loss:  0.143379807472229\n",
      "epoch: 25   step: 201   recon_loss: 0.007852528244256973   perplexity:  652.9494018554688 \n",
      "\t\tcommit_loss:  0.02543935552239418   codebook loss:  0.10175742208957672   total_loss:  0.13504931330680847\n",
      "epoch: 25   step: 251   recon_loss: 0.007933249697089195   perplexity:  653.0736694335938 \n",
      "\t\tcommit_loss:  0.026189742609858513   codebook loss:  0.10475897043943405   total_loss:  0.13888196647167206\n",
      "epoch: 25   step: 301   recon_loss: 0.007900562137365341   perplexity:  658.4736328125 \n",
      "\t\tcommit_loss:  0.026303840801119804   codebook loss:  0.10521536320447922   total_loss:  0.1394197642803192\n",
      "epoch: 25   step: 351   recon_loss: 0.008160490542650223   perplexity:  647.9070434570312 \n",
      "\t\tcommit_loss:  0.027020525187253952   codebook loss:  0.10808210074901581   total_loss:  0.14326311647891998\n",
      "epoch: 26   step: 1   recon_loss: 0.007659326307475567   perplexity:  646.6884765625 \n",
      "\t\tcommit_loss:  0.02514643408358097   codebook loss:  0.10058573633432388   total_loss:  0.13339149951934814\n",
      "epoch: 26   step: 51   recon_loss: 0.008661702275276184   perplexity:  658.0166015625 \n",
      "\t\tcommit_loss:  0.028533661738038063   codebook loss:  0.11413464695215225   total_loss:  0.15133000910282135\n",
      "epoch: 26   step: 101   recon_loss: 0.007581281941384077   perplexity:  637.968017578125 \n",
      "\t\tcommit_loss:  0.0249655619263649   codebook loss:  0.0998622477054596   total_loss:  0.13240909576416016\n",
      "epoch: 26   step: 151   recon_loss: 0.0076797958463430405   perplexity:  631.4432373046875 \n",
      "\t\tcommit_loss:  0.0242462120950222   codebook loss:  0.0969848483800888   total_loss:  0.1289108544588089\n",
      "epoch: 26   step: 201   recon_loss: 0.0080111650750041   perplexity:  654.2322387695312 \n",
      "\t\tcommit_loss:  0.025870617479085922   codebook loss:  0.10348246991634369   total_loss:  0.1373642534017563\n",
      "epoch: 26   step: 251   recon_loss: 0.008262673392891884   perplexity:  655.55126953125 \n",
      "\t\tcommit_loss:  0.02759460173547268   codebook loss:  0.11037840694189072   total_loss:  0.14623567461967468\n",
      "epoch: 26   step: 301   recon_loss: 0.008089250884950161   perplexity:  653.9221801757812 \n",
      "\t\tcommit_loss:  0.026633121073246002   codebook loss:  0.10653248429298401   total_loss:  0.14125485718250275\n",
      "epoch: 26   step: 351   recon_loss: 0.007846375927329063   perplexity:  646.6699829101562 \n",
      "\t\tcommit_loss:  0.025240065529942513   codebook loss:  0.10096026211977005   total_loss:  0.13404670357704163\n",
      "epoch: 27   step: 1   recon_loss: 0.007652594707906246   perplexity:  640.8013305664062 \n",
      "\t\tcommit_loss:  0.025388944894075394   codebook loss:  0.10155577957630157   total_loss:  0.1345973163843155\n",
      "epoch: 27   step: 51   recon_loss: 0.00767410546541214   perplexity:  636.69287109375 \n",
      "\t\tcommit_loss:  0.024877753108739853   codebook loss:  0.09951101243495941   total_loss:  0.1320628672838211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27   step: 101   recon_loss: 0.007737928535789251   perplexity:  644.4732666015625 \n",
      "\t\tcommit_loss:  0.02489336207509041   codebook loss:  0.09957344830036163   total_loss:  0.13220474123954773\n",
      "epoch: 27   step: 151   recon_loss: 0.007930881343781948   perplexity:  638.597412109375 \n",
      "\t\tcommit_loss:  0.02539108879864216   codebook loss:  0.10156435519456863   total_loss:  0.13488632440567017\n",
      "epoch: 27   step: 201   recon_loss: 0.008079132996499538   perplexity:  649.9195556640625 \n",
      "\t\tcommit_loss:  0.02673795446753502   codebook loss:  0.10695181787014008   total_loss:  0.1417689025402069\n",
      "epoch: 27   step: 251   recon_loss: 0.00820082612335682   perplexity:  641.2607421875 \n",
      "\t\tcommit_loss:  0.026279479265213013   codebook loss:  0.10511791706085205   total_loss:  0.13959822058677673\n",
      "epoch: 27   step: 301   recon_loss: 0.008126657456159592   perplexity:  641.4671630859375 \n",
      "\t\tcommit_loss:  0.02623705193400383   codebook loss:  0.10494820773601532   total_loss:  0.13931190967559814\n",
      "epoch: 27   step: 351   recon_loss: 0.00809505395591259   perplexity:  642.5812377929688 \n",
      "\t\tcommit_loss:  0.026136178523302078   codebook loss:  0.10454471409320831   total_loss:  0.13877594470977783\n",
      "epoch: 28   step: 1   recon_loss: 0.0074269771575927734   perplexity:  635.546630859375 \n",
      "\t\tcommit_loss:  0.02499726042151451   codebook loss:  0.09998904168605804   total_loss:  0.13241328299045563\n",
      "epoch: 28   step: 51   recon_loss: 0.0075682844035327435   perplexity:  645.8330078125 \n",
      "\t\tcommit_loss:  0.02536698803305626   codebook loss:  0.10146795213222504   total_loss:  0.13440322875976562\n",
      "epoch: 28   step: 101   recon_loss: 0.008201198652386665   perplexity:  659.44677734375 \n",
      "\t\tcommit_loss:  0.026823265478014946   codebook loss:  0.10729306191205978   total_loss:  0.142317533493042\n",
      "epoch: 28   step: 151   recon_loss: 0.007442086935043335   perplexity:  649.0316772460938 \n",
      "\t\tcommit_loss:  0.02609175443649292   codebook loss:  0.10436701774597168   total_loss:  0.13790085911750793\n",
      "epoch: 28   step: 201   recon_loss: 0.007895983755588531   perplexity:  651.297607421875 \n",
      "\t\tcommit_loss:  0.026368681341409683   codebook loss:  0.10547472536563873   total_loss:  0.13973939418792725\n",
      "epoch: 28   step: 251   recon_loss: 0.007916176691651344   perplexity:  653.4798583984375 \n",
      "\t\tcommit_loss:  0.02617882937192917   codebook loss:  0.10471531748771667   total_loss:  0.13881032168865204\n",
      "epoch: 28   step: 301   recon_loss: 0.008379001170396805   perplexity:  659.332275390625 \n",
      "\t\tcommit_loss:  0.026587795466184616   codebook loss:  0.10635118186473846   total_loss:  0.14131797850131989\n",
      "epoch: 28   step: 351   recon_loss: 0.007517683785408735   perplexity:  643.6502685546875 \n",
      "\t\tcommit_loss:  0.024683240801095963   codebook loss:  0.09873296320438385   total_loss:  0.13093388080596924\n",
      "epoch: 29   step: 1   recon_loss: 0.007878454402089119   perplexity:  661.7598266601562 \n",
      "\t\tcommit_loss:  0.026640575379133224   codebook loss:  0.1065623015165329   total_loss:  0.1410813331604004\n",
      "epoch: 29   step: 51   recon_loss: 0.008080129511654377   perplexity:  647.8032836914062 \n",
      "\t\tcommit_loss:  0.02675583027303219   codebook loss:  0.10702332109212875   total_loss:  0.1418592780828476\n",
      "epoch: 29   step: 101   recon_loss: 0.008621014654636383   perplexity:  655.1000366210938 \n",
      "\t\tcommit_loss:  0.02794702537357807   codebook loss:  0.11178810149431229   total_loss:  0.1483561396598816\n",
      "epoch: 29   step: 151   recon_loss: 0.008234690874814987   perplexity:  653.5880126953125 \n",
      "\t\tcommit_loss:  0.026911888271570206   codebook loss:  0.10764755308628082   total_loss:  0.14279413223266602\n",
      "epoch: 29   step: 201   recon_loss: 0.008031831122934818   perplexity:  649.6605834960938 \n",
      "\t\tcommit_loss:  0.02639182098209858   codebook loss:  0.10556728392839432   total_loss:  0.1399909406900406\n",
      "epoch: 29   step: 251   recon_loss: 0.00791095569729805   perplexity:  648.2164306640625 \n",
      "\t\tcommit_loss:  0.025790702551603317   codebook loss:  0.10316281020641327   total_loss:  0.13686446845531464\n",
      "epoch: 29   step: 301   recon_loss: 0.007528399582952261   perplexity:  653.1218872070312 \n",
      "\t\tcommit_loss:  0.02562130242586136   codebook loss:  0.10248520970344543   total_loss:  0.1356349140405655\n",
      "epoch: 29   step: 351   recon_loss: 0.007778453640639782   perplexity:  650.8790893554688 \n",
      "\t\tcommit_loss:  0.026689700782299042   codebook loss:  0.10675880312919617   total_loss:  0.14122696220874786\n",
      "epoch: 30   step: 1   recon_loss: 0.008031689561903477   perplexity:  644.5350341796875 \n",
      "\t\tcommit_loss:  0.02617197483778   codebook loss:  0.10468789935112   total_loss:  0.1388915628194809\n",
      "epoch: 30   step: 51   recon_loss: 0.008192135021090508   perplexity:  658.13330078125 \n",
      "\t\tcommit_loss:  0.026674946770071983   codebook loss:  0.10669978708028793   total_loss:  0.14156687259674072\n",
      "epoch: 30   step: 101   recon_loss: 0.00794211309403181   perplexity:  657.4927978515625 \n",
      "\t\tcommit_loss:  0.026476213708519936   codebook loss:  0.10590485483407974   total_loss:  0.14032317698001862\n",
      "epoch: 30   step: 151   recon_loss: 0.007772579789161682   perplexity:  651.0861206054688 \n",
      "\t\tcommit_loss:  0.02609354630112648   codebook loss:  0.10437418520450592   total_loss:  0.13824030756950378\n",
      "epoch: 30   step: 201   recon_loss: 0.007801324129104614   perplexity:  654.1738891601562 \n",
      "\t\tcommit_loss:  0.026614505797624588   codebook loss:  0.10645802319049835   total_loss:  0.14087384939193726\n",
      "epoch: 30   step: 251   recon_loss: 0.008099893108010292   perplexity:  652.72216796875 \n",
      "\t\tcommit_loss:  0.026981394737958908   codebook loss:  0.10792557895183563   total_loss:  0.14300686120986938\n",
      "epoch: 30   step: 301   recon_loss: 0.007979312911629677   perplexity:  651.76611328125 \n",
      "\t\tcommit_loss:  0.026719743385910988   codebook loss:  0.10687897354364395   total_loss:  0.14157803356647491\n",
      "epoch: 30   step: 351   recon_loss: 0.007818235084414482   perplexity:  652.8681640625 \n",
      "\t\tcommit_loss:  0.02622918039560318   codebook loss:  0.10491672158241272   total_loss:  0.13896414637565613\n",
      "epoch: 31   step: 1   recon_loss: 0.00787821039557457   perplexity:  652.4299926757812 \n",
      "\t\tcommit_loss:  0.02591681480407715   codebook loss:  0.1036672592163086   total_loss:  0.1374622881412506\n",
      "epoch: 31   step: 51   recon_loss: 0.007458855863660574   perplexity:  643.1448974609375 \n",
      "\t\tcommit_loss:  0.025242842733860016   codebook loss:  0.10097137093544006   total_loss:  0.1336730718612671\n",
      "epoch: 31   step: 101   recon_loss: 0.00791163370013237   perplexity:  659.6430053710938 \n",
      "\t\tcommit_loss:  0.026810839772224426   codebook loss:  0.1072433590888977   total_loss:  0.1419658362865448\n",
      "epoch: 31   step: 151   recon_loss: 0.008146011270582676   perplexity:  660.0538940429688 \n",
      "\t\tcommit_loss:  0.027291007339954376   codebook loss:  0.1091640293598175   total_loss:  0.14460104703903198\n",
      "epoch: 31   step: 201   recon_loss: 0.0076545472256839275   perplexity:  645.777587890625 \n",
      "\t\tcommit_loss:  0.025478100404143333   codebook loss:  0.10191240161657333   total_loss:  0.13504505157470703\n",
      "epoch: 31   step: 251   recon_loss: 0.008381165564060211   perplexity:  663.3115844726562 \n",
      "\t\tcommit_loss:  0.028734315186738968   codebook loss:  0.11493726074695587   total_loss:  0.15205274522304535\n",
      "epoch: 31   step: 301   recon_loss: 0.0081791952252388   perplexity:  649.8067626953125 \n",
      "\t\tcommit_loss:  0.028397679328918457   codebook loss:  0.11359071731567383   total_loss:  0.1501675844192505\n",
      "epoch: 31   step: 351   recon_loss: 0.007578689604997635   perplexity:  661.5496826171875 \n",
      "\t\tcommit_loss:  0.025658641010522842   codebook loss:  0.10263456404209137   total_loss:  0.13587188720703125\n",
      "epoch: 32   step: 1   recon_loss: 0.007780707906931639   perplexity:  643.953857421875 \n",
      "\t\tcommit_loss:  0.02621198073029518   codebook loss:  0.10484792292118073   total_loss:  0.13884061574935913\n",
      "epoch: 32   step: 51   recon_loss: 0.008122481405735016   perplexity:  659.044677734375 \n",
      "\t\tcommit_loss:  0.028370346873998642   codebook loss:  0.11348138749599457   total_loss:  0.14997421205043793\n",
      "epoch: 32   step: 101   recon_loss: 0.008182775229215622   perplexity:  658.63818359375 \n",
      "\t\tcommit_loss:  0.027231942862272263   codebook loss:  0.10892777144908905   total_loss:  0.14434248208999634\n",
      "epoch: 32   step: 151   recon_loss: 0.008087961003184319   perplexity:  665.57568359375 \n",
      "\t\tcommit_loss:  0.026963982731103897   codebook loss:  0.10785593092441559   total_loss:  0.14290787279605865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32   step: 201   recon_loss: 0.008173206821084023   perplexity:  648.2000122070312 \n",
      "\t\tcommit_loss:  0.027290374040603638   codebook loss:  0.10916149616241455   total_loss:  0.14462506771087646\n",
      "epoch: 32   step: 251   recon_loss: 0.00721497368067503   perplexity:  620.0120849609375 \n",
      "\t\tcommit_loss:  0.023767128586769104   codebook loss:  0.09506851434707642   total_loss:  0.12605062127113342\n",
      "epoch: 32   step: 301   recon_loss: 0.007658489979803562   perplexity:  652.1416015625 \n",
      "\t\tcommit_loss:  0.025979500263929367   codebook loss:  0.10391800105571747   total_loss:  0.13755598664283752\n",
      "epoch: 32   step: 351   recon_loss: 0.008209575898945332   perplexity:  668.3692626953125 \n",
      "\t\tcommit_loss:  0.027482468634843826   codebook loss:  0.1099298745393753   total_loss:  0.14562192559242249\n",
      "epoch: 33   step: 1   recon_loss: 0.007825877517461777   perplexity:  644.1414794921875 \n",
      "\t\tcommit_loss:  0.02651306800544262   codebook loss:  0.10605227202177048   total_loss:  0.14039121568202972\n",
      "epoch: 33   step: 51   recon_loss: 0.008105652406811714   perplexity:  658.8781127929688 \n",
      "\t\tcommit_loss:  0.02688869833946228   codebook loss:  0.10755479335784912   total_loss:  0.14254914224147797\n",
      "epoch: 33   step: 101   recon_loss: 0.00804204773157835   perplexity:  656.348876953125 \n",
      "\t\tcommit_loss:  0.027140166610479355   codebook loss:  0.10856066644191742   total_loss:  0.1437428891658783\n",
      "epoch: 33   step: 151   recon_loss: 0.007131281774491072   perplexity:  632.862060546875 \n",
      "\t\tcommit_loss:  0.024308906868100166   codebook loss:  0.09723562747240067   total_loss:  0.12867581844329834\n",
      "epoch: 33   step: 201   recon_loss: 0.008076198399066925   perplexity:  661.2254638671875 \n",
      "\t\tcommit_loss:  0.028269503265619278   codebook loss:  0.11307801306247711   total_loss:  0.1494237184524536\n",
      "epoch: 33   step: 251   recon_loss: 0.008146143518388271   perplexity:  660.455322265625 \n",
      "\t\tcommit_loss:  0.028045639395713806   codebook loss:  0.11218255758285522   total_loss:  0.14837434887886047\n",
      "epoch: 33   step: 301   recon_loss: 0.007847405038774014   perplexity:  657.4890747070312 \n",
      "\t\tcommit_loss:  0.026318449527025223   codebook loss:  0.10527379810810089   total_loss:  0.139439657330513\n",
      "epoch: 33   step: 351   recon_loss: 0.007721440400928259   perplexity:  651.528076171875 \n",
      "\t\tcommit_loss:  0.025492940098047256   codebook loss:  0.10197176039218903   total_loss:  0.13518613576889038\n",
      "epoch: 34   step: 1   recon_loss: 0.00817358959466219   perplexity:  658.0303955078125 \n",
      "\t\tcommit_loss:  0.027050435543060303   codebook loss:  0.10820174217224121   total_loss:  0.14342576265335083\n",
      "epoch: 34   step: 51   recon_loss: 0.00752747617661953   perplexity:  631.6537475585938 \n",
      "\t\tcommit_loss:  0.024705592542886734   codebook loss:  0.09882237017154694   total_loss:  0.13105544447898865\n",
      "epoch: 34   step: 101   recon_loss: 0.007674628868699074   perplexity:  650.0187377929688 \n",
      "\t\tcommit_loss:  0.026359297335147858   codebook loss:  0.10543718934059143   total_loss:  0.1394711136817932\n",
      "epoch: 34   step: 151   recon_loss: 0.007311326451599598   perplexity:  640.6329956054688 \n",
      "\t\tcommit_loss:  0.02441955916583538   codebook loss:  0.09767823666334152   total_loss:  0.12940911948680878\n",
      "epoch: 34   step: 201   recon_loss: 0.007828638888895512   perplexity:  636.9849853515625 \n",
      "\t\tcommit_loss:  0.026264112442731857   codebook loss:  0.10505644977092743   total_loss:  0.13914920389652252\n",
      "epoch: 34   step: 251   recon_loss: 0.007900601252913475   perplexity:  660.3246459960938 \n",
      "\t\tcommit_loss:  0.02740788459777832   codebook loss:  0.10963153839111328   total_loss:  0.14494001865386963\n",
      "epoch: 34   step: 301   recon_loss: 0.00754574267193675   perplexity:  647.0598754882812 \n",
      "\t\tcommit_loss:  0.02604408748447895   codebook loss:  0.1041763499379158   total_loss:  0.13776618242263794\n",
      "epoch: 34   step: 351   recon_loss: 0.0075235627591609955   perplexity:  654.4593505859375 \n",
      "\t\tcommit_loss:  0.02598220854997635   codebook loss:  0.1039288341999054   total_loss:  0.13743460178375244\n",
      "epoch: 35   step: 1   recon_loss: 0.007600072305649519   perplexity:  650.48876953125 \n",
      "\t\tcommit_loss:  0.025940487161278725   codebook loss:  0.1037619486451149   total_loss:  0.13730250298976898\n",
      "epoch: 35   step: 51   recon_loss: 0.007256205193698406   perplexity:  640.3397827148438 \n",
      "\t\tcommit_loss:  0.024616096168756485   codebook loss:  0.09846438467502594   total_loss:  0.1303366869688034\n",
      "epoch: 35   step: 101   recon_loss: 0.007306351326406002   perplexity:  648.3292236328125 \n",
      "\t\tcommit_loss:  0.025850865989923477   codebook loss:  0.10340346395969391   total_loss:  0.13656067848205566\n",
      "epoch: 35   step: 151   recon_loss: 0.007355019450187683   perplexity:  639.3646240234375 \n",
      "\t\tcommit_loss:  0.02476726844906807   codebook loss:  0.09906907379627228   total_loss:  0.13119135797023773\n",
      "epoch: 35   step: 201   recon_loss: 0.007413040846586227   perplexity:  642.1958618164062 \n",
      "\t\tcommit_loss:  0.02464001439511776   codebook loss:  0.09856005758047104   total_loss:  0.13061311841011047\n",
      "epoch: 35   step: 251   recon_loss: 0.007991407066583633   perplexity:  648.647705078125 \n",
      "\t\tcommit_loss:  0.02654537558555603   codebook loss:  0.10618150234222412   total_loss:  0.1407182812690735\n",
      "epoch: 35   step: 301   recon_loss: 0.007716377731412649   perplexity:  653.1104125976562 \n",
      "\t\tcommit_loss:  0.02659112587571144   codebook loss:  0.10636450350284576   total_loss:  0.1406719982624054\n",
      "epoch: 35   step: 351   recon_loss: 0.00826510414481163   perplexity:  654.95947265625 \n",
      "\t\tcommit_loss:  0.027241963893175125   codebook loss:  0.1089678555727005   total_loss:  0.14447492361068726\n",
      "epoch: 36   step: 1   recon_loss: 0.007963746786117554   perplexity:  660.9285278320312 \n",
      "\t\tcommit_loss:  0.02706705406308174   codebook loss:  0.10826821625232697   total_loss:  0.14329901337623596\n",
      "epoch: 36   step: 51   recon_loss: 0.007651461288332939   perplexity:  647.468505859375 \n",
      "\t\tcommit_loss:  0.02490166947245598   codebook loss:  0.09960667788982391   total_loss:  0.13215979933738708\n",
      "epoch: 36   step: 101   recon_loss: 0.007465863134711981   perplexity:  636.1802978515625 \n",
      "\t\tcommit_loss:  0.025069361552596092   codebook loss:  0.10027744621038437   total_loss:  0.13281267881393433\n",
      "epoch: 36   step: 151   recon_loss: 0.007359206210821867   perplexity:  645.8865966796875 \n",
      "\t\tcommit_loss:  0.026003748178482056   codebook loss:  0.10401499271392822   total_loss:  0.13737794756889343\n",
      "epoch: 36   step: 201   recon_loss: 0.00772804394364357   perplexity:  654.7528076171875 \n",
      "\t\tcommit_loss:  0.02623296156525612   codebook loss:  0.10493184626102448   total_loss:  0.13889285922050476\n",
      "epoch: 36   step: 251   recon_loss: 0.007985992357134819   perplexity:  661.2374267578125 \n",
      "\t\tcommit_loss:  0.027846910059452057   codebook loss:  0.11138764023780823   total_loss:  0.14722055196762085\n",
      "epoch: 36   step: 301   recon_loss: 0.007901810109615326   perplexity:  653.4823608398438 \n",
      "\t\tcommit_loss:  0.026932505890727043   codebook loss:  0.10773002356290817   total_loss:  0.1425643414258957\n",
      "epoch: 36   step: 351   recon_loss: 0.007487195543944836   perplexity:  649.4031372070312 \n",
      "\t\tcommit_loss:  0.025825057178735733   codebook loss:  0.10330022871494293   total_loss:  0.13661247491836548\n",
      "epoch: 37   step: 1   recon_loss: 0.007468267809599638   perplexity:  650.0819702148438 \n",
      "\t\tcommit_loss:  0.025761522352695465   codebook loss:  0.10304608941078186   total_loss:  0.13627588748931885\n",
      "epoch: 37   step: 51   recon_loss: 0.007649106439203024   perplexity:  663.6071166992188 \n",
      "\t\tcommit_loss:  0.02550640143454075   codebook loss:  0.102025605738163   total_loss:  0.13518111407756805\n",
      "epoch: 37   step: 101   recon_loss: 0.007384940050542355   perplexity:  645.6458129882812 \n",
      "\t\tcommit_loss:  0.025324970483779907   codebook loss:  0.10129988193511963   total_loss:  0.13400979340076447\n",
      "epoch: 37   step: 151   recon_loss: 0.007591730915009975   perplexity:  649.9725341796875 \n",
      "\t\tcommit_loss:  0.025772590190172195   codebook loss:  0.10309036076068878   total_loss:  0.13645468652248383\n",
      "epoch: 37   step: 201   recon_loss: 0.00796673633158207   perplexity:  662.4088745117188 \n",
      "\t\tcommit_loss:  0.027765413746237755   codebook loss:  0.11106165498495102   total_loss:  0.14679381251335144\n",
      "epoch: 37   step: 251   recon_loss: 0.007770649157464504   perplexity:  662.1300659179688 \n",
      "\t\tcommit_loss:  0.026863880455493927   codebook loss:  0.10745552182197571   total_loss:  0.1420900523662567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37   step: 301   recon_loss: 0.007290534675121307   perplexity:  656.4033203125 \n",
      "\t\tcommit_loss:  0.02561289817094803   codebook loss:  0.10245159268379211   total_loss:  0.13535502552986145\n",
      "epoch: 37   step: 351   recon_loss: 0.007722970098257065   perplexity:  659.7182006835938 \n",
      "\t\tcommit_loss:  0.027155006304383278   codebook loss:  0.10862002521753311   total_loss:  0.1434980034828186\n",
      "epoch: 38   step: 1   recon_loss: 0.0070112599059939384   perplexity:  637.5927124023438 \n",
      "\t\tcommit_loss:  0.02407214045524597   codebook loss:  0.09628856182098389   total_loss:  0.12737196683883667\n",
      "epoch: 38   step: 51   recon_loss: 0.007578765973448753   perplexity:  649.732421875 \n",
      "\t\tcommit_loss:  0.026304941624403   codebook loss:  0.105219766497612   total_loss:  0.1391034722328186\n",
      "epoch: 38   step: 101   recon_loss: 0.007442460395395756   perplexity:  650.87353515625 \n",
      "\t\tcommit_loss:  0.024920472875237465   codebook loss:  0.09968189150094986   total_loss:  0.13204482197761536\n",
      "epoch: 38   step: 151   recon_loss: 0.00738298986107111   perplexity:  651.4404907226562 \n",
      "\t\tcommit_loss:  0.02575092762708664   codebook loss:  0.10300371050834656   total_loss:  0.13613763451576233\n",
      "epoch: 38   step: 201   recon_loss: 0.007377392612397671   perplexity:  654.4637451171875 \n",
      "\t\tcommit_loss:  0.026077616959810257   codebook loss:  0.10431046783924103   total_loss:  0.13776548206806183\n",
      "epoch: 38   step: 251   recon_loss: 0.007110055536031723   perplexity:  641.3267822265625 \n",
      "\t\tcommit_loss:  0.02477297931909561   codebook loss:  0.09909191727638245   total_loss:  0.13097494840621948\n",
      "epoch: 38   step: 301   recon_loss: 0.007454502861946821   perplexity:  652.3326416015625 \n",
      "\t\tcommit_loss:  0.026019005104899406   codebook loss:  0.10407602041959763   total_loss:  0.1375495195388794\n",
      "epoch: 38   step: 351   recon_loss: 0.007734400685876608   perplexity:  661.2841186523438 \n",
      "\t\tcommit_loss:  0.027491752058267593   codebook loss:  0.10996700823307037   total_loss:  0.1451931595802307\n",
      "epoch: 39   step: 1   recon_loss: 0.007584325037896633   perplexity:  649.5385131835938 \n",
      "\t\tcommit_loss:  0.02643395960330963   codebook loss:  0.10573583841323853   total_loss:  0.13975411653518677\n",
      "epoch: 39   step: 51   recon_loss: 0.007847052998840809   perplexity:  659.274169921875 \n",
      "\t\tcommit_loss:  0.027395296841859818   codebook loss:  0.10958118736743927   total_loss:  0.14482353627681732\n",
      "epoch: 39   step: 101   recon_loss: 0.007925432175397873   perplexity:  654.4774780273438 \n",
      "\t\tcommit_loss:  0.027788791805505753   codebook loss:  0.11115516722202301   total_loss:  0.14686939120292664\n",
      "epoch: 39   step: 151   recon_loss: 0.007738748099654913   perplexity:  660.4385986328125 \n",
      "\t\tcommit_loss:  0.026351673528552055   codebook loss:  0.10540669411420822   total_loss:  0.13949711620807648\n",
      "epoch: 39   step: 201   recon_loss: 0.007470026146620512   perplexity:  658.5828857421875 \n",
      "\t\tcommit_loss:  0.026077333837747574   codebook loss:  0.1043093353509903   total_loss:  0.13785669207572937\n",
      "epoch: 39   step: 251   recon_loss: 0.007395529188215733   perplexity:  643.9658203125 \n",
      "\t\tcommit_loss:  0.024689121171832085   codebook loss:  0.09875648468732834   total_loss:  0.13084113597869873\n",
      "epoch: 39   step: 301   recon_loss: 0.007508664391934872   perplexity:  647.993896484375 \n",
      "\t\tcommit_loss:  0.025392815470695496   codebook loss:  0.10157126188278198   total_loss:  0.13447274267673492\n",
      "epoch: 39   step: 351   recon_loss: 0.007430519908666611   perplexity:  648.853759765625 \n",
      "\t\tcommit_loss:  0.025486798956990242   codebook loss:  0.10194719582796097   total_loss:  0.13486450910568237\n",
      "epoch: 40   step: 1   recon_loss: 0.007707198616117239   perplexity:  655.3465576171875 \n",
      "\t\tcommit_loss:  0.026701632887125015   codebook loss:  0.10680653154850006   total_loss:  0.14121535420417786\n",
      "epoch: 40   step: 51   recon_loss: 0.007564305327832699   perplexity:  643.3743896484375 \n",
      "\t\tcommit_loss:  0.026081174612045288   codebook loss:  0.10432469844818115   total_loss:  0.1379701793193817\n",
      "epoch: 40   step: 101   recon_loss: 0.007548385765403509   perplexity:  653.6970825195312 \n",
      "\t\tcommit_loss:  0.025921810418367386   codebook loss:  0.10368724167346954   total_loss:  0.13715744018554688\n",
      "epoch: 40   step: 151   recon_loss: 0.007859419099986553   perplexity:  659.6364135742188 \n",
      "\t\tcommit_loss:  0.02772391214966774   codebook loss:  0.11089564859867096   total_loss:  0.14647898077964783\n",
      "epoch: 40   step: 201   recon_loss: 0.007326024584472179   perplexity:  640.3452758789062 \n",
      "\t\tcommit_loss:  0.026030469685792923   codebook loss:  0.10412187874317169   total_loss:  0.13747838139533997\n",
      "epoch: 40   step: 251   recon_loss: 0.007728289812803268   perplexity:  657.1417846679688 \n",
      "\t\tcommit_loss:  0.026353642344474792   codebook loss:  0.10541456937789917   total_loss:  0.13949650526046753\n",
      "epoch: 40   step: 301   recon_loss: 0.008186154067516327   perplexity:  658.2042236328125 \n",
      "\t\tcommit_loss:  0.027455095201730728   codebook loss:  0.10982038080692291   total_loss:  0.14546163380146027\n",
      "epoch: 40   step: 351   recon_loss: 0.008223076350986958   perplexity:  664.4442749023438 \n",
      "\t\tcommit_loss:  0.027799654752016068   codebook loss:  0.11119861900806427   total_loss:  0.14722135663032532\n",
      "epoch: 41   step: 1   recon_loss: 0.007247847970575094   perplexity:  639.9850463867188 \n",
      "\t\tcommit_loss:  0.02586577832698822   codebook loss:  0.10346311330795288   total_loss:  0.13657674193382263\n",
      "epoch: 41   step: 51   recon_loss: 0.0066494932398200035   perplexity:  636.1077880859375 \n",
      "\t\tcommit_loss:  0.0246258731931448   codebook loss:  0.0985034927725792   total_loss:  0.12977886199951172\n",
      "epoch: 41   step: 101   recon_loss: 0.007413115352392197   perplexity:  657.5455322265625 \n",
      "\t\tcommit_loss:  0.02603861689567566   codebook loss:  0.10415446758270264   total_loss:  0.1376062035560608\n",
      "epoch: 41   step: 151   recon_loss: 0.007240216713398695   perplexity:  638.40625 \n",
      "\t\tcommit_loss:  0.025025565177202225   codebook loss:  0.1001022607088089   total_loss:  0.1323680430650711\n",
      "epoch: 41   step: 201   recon_loss: 0.0071659572422504425   perplexity:  655.3372192382812 \n",
      "\t\tcommit_loss:  0.025167588144540787   codebook loss:  0.10067035257816315   total_loss:  0.13300389051437378\n",
      "epoch: 41   step: 251   recon_loss: 0.007202418055385351   perplexity:  652.9973754882812 \n",
      "\t\tcommit_loss:  0.02522389590740204   codebook loss:  0.10089558362960815   total_loss:  0.13332189619541168\n",
      "epoch: 41   step: 301   recon_loss: 0.007749214768409729   perplexity:  656.847900390625 \n",
      "\t\tcommit_loss:  0.027469780296087265   codebook loss:  0.10987912118434906   total_loss:  0.14509811997413635\n",
      "epoch: 41   step: 351   recon_loss: 0.0072989496402442455   perplexity:  656.07666015625 \n",
      "\t\tcommit_loss:  0.02524542808532715   codebook loss:  0.1009817123413086   total_loss:  0.13352608680725098\n",
      "epoch: 42   step: 1   recon_loss: 0.006998253520578146   perplexity:  644.994384765625 \n",
      "\t\tcommit_loss:  0.024927418678998947   codebook loss:  0.09970967471599579   total_loss:  0.13163533806800842\n",
      "epoch: 42   step: 51   recon_loss: 0.006945577450096607   perplexity:  627.4692993164062 \n",
      "\t\tcommit_loss:  0.023530684411525726   codebook loss:  0.0941227376461029   total_loss:  0.12459899485111237\n",
      "epoch: 42   step: 101   recon_loss: 0.008014868013560772   perplexity:  662.6768188476562 \n",
      "\t\tcommit_loss:  0.028341324999928474   codebook loss:  0.1133652999997139   total_loss:  0.14972148835659027\n",
      "epoch: 42   step: 151   recon_loss: 0.0076195537112653255   perplexity:  654.2914428710938 \n",
      "\t\tcommit_loss:  0.0266408808529377   codebook loss:  0.1065635234117508   total_loss:  0.14082396030426025\n",
      "epoch: 42   step: 201   recon_loss: 0.007500545587390661   perplexity:  655.3218994140625 \n",
      "\t\tcommit_loss:  0.02657485008239746   codebook loss:  0.10629940032958984   total_loss:  0.1403747946023941\n",
      "epoch: 42   step: 251   recon_loss: 0.007489558309316635   perplexity:  666.0595092773438 \n",
      "\t\tcommit_loss:  0.026900669559836388   codebook loss:  0.10760267823934555   total_loss:  0.14199289679527283\n",
      "epoch: 42   step: 301   recon_loss: 0.007200729101896286   perplexity:  637.621337890625 \n",
      "\t\tcommit_loss:  0.02477216348052025   codebook loss:  0.099088653922081   total_loss:  0.13106155395507812\n",
      "epoch: 42   step: 351   recon_loss: 0.007331253029406071   perplexity:  654.3763427734375 \n",
      "\t\tcommit_loss:  0.026208028197288513   codebook loss:  0.10483211278915405   total_loss:  0.13837139308452606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43   step: 1   recon_loss: 0.007287102751433849   perplexity:  651.2134399414062 \n",
      "\t\tcommit_loss:  0.025362657383084297   codebook loss:  0.10145062953233719   total_loss:  0.13410039246082306\n",
      "epoch: 43   step: 51   recon_loss: 0.007483947090804577   perplexity:  658.1427612304688 \n",
      "\t\tcommit_loss:  0.0265862625092268   codebook loss:  0.1063450500369072   total_loss:  0.1404152512550354\n",
      "epoch: 43   step: 101   recon_loss: 0.008311585523188114   perplexity:  664.3536376953125 \n",
      "\t\tcommit_loss:  0.0280187726020813   codebook loss:  0.1120750904083252   total_loss:  0.14840544760227203\n",
      "epoch: 43   step: 151   recon_loss: 0.007555972784757614   perplexity:  650.3740234375 \n",
      "\t\tcommit_loss:  0.025769811123609543   codebook loss:  0.10307924449443817   total_loss:  0.13640502095222473\n",
      "epoch: 43   step: 201   recon_loss: 0.007470604032278061   perplexity:  659.9151611328125 \n",
      "\t\tcommit_loss:  0.025891004130244255   codebook loss:  0.10356401652097702   total_loss:  0.1369256228208542\n",
      "epoch: 43   step: 251   recon_loss: 0.007540612947195768   perplexity:  650.2192993164062 \n",
      "\t\tcommit_loss:  0.02575240097939968   codebook loss:  0.10300960391759872   total_loss:  0.1363026201725006\n",
      "epoch: 43   step: 301   recon_loss: 0.007814949378371239   perplexity:  647.1456298828125 \n",
      "\t\tcommit_loss:  0.025721311569213867   codebook loss:  0.10288524627685547   total_loss:  0.13642150163650513\n",
      "epoch: 43   step: 351   recon_loss: 0.007282716687768698   perplexity:  648.0469970703125 \n",
      "\t\tcommit_loss:  0.025643695145845413   codebook loss:  0.10257478058338165   total_loss:  0.1355011910200119\n",
      "epoch: 44   step: 1   recon_loss: 0.007566608488559723   perplexity:  649.6431884765625 \n",
      "\t\tcommit_loss:  0.026115387678146362   codebook loss:  0.10446155071258545   total_loss:  0.13814353942871094\n",
      "epoch: 44   step: 51   recon_loss: 0.007432518061250448   perplexity:  646.3974609375 \n",
      "\t\tcommit_loss:  0.0255812369287014   codebook loss:  0.1023249477148056   total_loss:  0.135338693857193\n",
      "epoch: 44   step: 101   recon_loss: 0.0072721876204013824   perplexity:  658.0633544921875 \n",
      "\t\tcommit_loss:  0.026154708117246628   codebook loss:  0.10461883246898651   total_loss:  0.13804572820663452\n",
      "epoch: 44   step: 151   recon_loss: 0.007809176575392485   perplexity:  648.1663208007812 \n",
      "\t\tcommit_loss:  0.026387516409158707   codebook loss:  0.10555006563663483   total_loss:  0.139746755361557\n",
      "epoch: 44   step: 201   recon_loss: 0.0076974257826805115   perplexity:  655.7894897460938 \n",
      "\t\tcommit_loss:  0.026393169537186623   codebook loss:  0.10557267814874649   total_loss:  0.13966327905654907\n",
      "epoch: 44   step: 251   recon_loss: 0.0072457753121852875   perplexity:  641.8602905273438 \n",
      "\t\tcommit_loss:  0.025228329002857208   codebook loss:  0.10091331601142883   total_loss:  0.13338741660118103\n",
      "epoch: 44   step: 301   recon_loss: 0.007850047200918198   perplexity:  644.9325561523438 \n",
      "\t\tcommit_loss:  0.026600543409585953   codebook loss:  0.10640217363834381   total_loss:  0.14085276424884796\n",
      "epoch: 44   step: 351   recon_loss: 0.007276034913957119   perplexity:  649.9325561523438 \n",
      "\t\tcommit_loss:  0.02550579234957695   codebook loss:  0.1020231693983078   total_loss:  0.13480499386787415\n",
      "epoch: 45   step: 1   recon_loss: 0.007611150853335857   perplexity:  646.8908081054688 \n",
      "\t\tcommit_loss:  0.026316124945878983   codebook loss:  0.10526449978351593   total_loss:  0.13919177651405334\n",
      "epoch: 45   step: 51   recon_loss: 0.007337233982980251   perplexity:  643.6134033203125 \n",
      "\t\tcommit_loss:  0.025937862694263458   codebook loss:  0.10375145077705383   total_loss:  0.13702654838562012\n",
      "epoch: 45   step: 101   recon_loss: 0.007554437033832073   perplexity:  656.5911254882812 \n",
      "\t\tcommit_loss:  0.02644549310207367   codebook loss:  0.10578197240829468   total_loss:  0.1397819072008133\n",
      "epoch: 45   step: 151   recon_loss: 0.007531723007559776   perplexity:  649.0171508789062 \n",
      "\t\tcommit_loss:  0.027096539735794067   codebook loss:  0.10838615894317627   total_loss:  0.14301443099975586\n",
      "epoch: 45   step: 201   recon_loss: 0.007290337234735489   perplexity:  650.7704467773438 \n",
      "\t\tcommit_loss:  0.025321058928966522   codebook loss:  0.10128423571586609   total_loss:  0.1338956356048584\n",
      "epoch: 45   step: 251   recon_loss: 0.007364058401435614   perplexity:  667.4061889648438 \n",
      "\t\tcommit_loss:  0.026665814220905304   codebook loss:  0.10666325688362122   total_loss:  0.14069312810897827\n",
      "epoch: 45   step: 301   recon_loss: 0.007300815545022488   perplexity:  652.5961303710938 \n",
      "\t\tcommit_loss:  0.02596292272210121   codebook loss:  0.10385169088840485   total_loss:  0.13711543381214142\n",
      "epoch: 45   step: 351   recon_loss: 0.007198636420071125   perplexity:  654.8201904296875 \n",
      "\t\tcommit_loss:  0.02537873014807701   codebook loss:  0.10151492059230804   total_loss:  0.1340922862291336\n",
      "epoch: 46   step: 1   recon_loss: 0.007381930015981197   perplexity:  654.2967529296875 \n",
      "\t\tcommit_loss:  0.025789767503738403   codebook loss:  0.10315907001495361   total_loss:  0.1363307684659958\n",
      "epoch: 46   step: 51   recon_loss: 0.007060717791318893   perplexity:  643.3096313476562 \n",
      "\t\tcommit_loss:  0.02541622705757618   codebook loss:  0.10166490823030472   total_loss:  0.13414186239242554\n",
      "epoch: 46   step: 101   recon_loss: 0.00750382523983717   perplexity:  651.5225219726562 \n",
      "\t\tcommit_loss:  0.026888661086559296   codebook loss:  0.10755464434623718   total_loss:  0.14194713532924652\n",
      "epoch: 46   step: 151   recon_loss: 0.007590099237859249   perplexity:  653.4490356445312 \n",
      "\t\tcommit_loss:  0.02713540568947792   codebook loss:  0.10854162275791168   total_loss:  0.14326712489128113\n",
      "epoch: 46   step: 201   recon_loss: 0.007427053526043892   perplexity:  650.1334228515625 \n",
      "\t\tcommit_loss:  0.026257922872900963   codebook loss:  0.10503169149160385   total_loss:  0.1387166678905487\n",
      "epoch: 46   step: 251   recon_loss: 0.0072952741757035255   perplexity:  652.6705322265625 \n",
      "\t\tcommit_loss:  0.02532358467578888   codebook loss:  0.10129433870315552   total_loss:  0.13391318917274475\n",
      "epoch: 46   step: 301   recon_loss: 0.007270443253219128   perplexity:  645.2269287109375 \n",
      "\t\tcommit_loss:  0.025539614260196686   codebook loss:  0.10215845704078674   total_loss:  0.13496851921081543\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training VQ-VAE...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, commitment_loss, codebook_loss, perplexity = model(x)\n",
    "        recon_loss = mse_loss(x_hat, x)\n",
    "        \n",
    "        loss =  recon_loss + commitment_loss + codebook_loss\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % print_step ==0: \n",
    "            print(\"epoch:\", epoch + 1, \"  step:\", batch_idx + 1, \"  recon_loss:\", recon_loss.item(), \"  perplexity: \", perplexity.item(), \n",
    "              \"\\n\\t\\tcommit_loss: \", commitment_loss.item(), \"  codebook loss: \", codebook_loss.item(), \"  total_loss: \", loss.item())\n",
    "    \n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_image(x, postfix):\n",
    "  \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Visualization of {}\".format(postfix))\n",
    "    plt.imshow(np.transpose(make_grid(x.detach().cpu(), padding=2, normalize=True), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "        x_hat, commitment_loss, codebook_loss, perplexity = model(x)\n",
    " \n",
    "        print(\"perplexity: \", perplexity.item(),\"commit_loss: \", commitment_loss.item(), \"  codebook loss: \", codebook_loss.item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample_image(x[:batch_size//2], \"Ground-truth images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample_image(x_hat[:batch_size//2], \"Reconstructed images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Generate samples via random codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_sample_image(codebook, decoder, indices_shape):\n",
    "    \n",
    "    random_indices = torch.floor(torch.rand(indices_shape) * n_embeddings).long().to(DEVICE)\n",
    "    codes = codebook.retrieve_random_codebook(random_indices)\n",
    "    x_hat = decoder(codes.to(DEVICE))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Visualization of Random Codes\")\n",
    "    plt.imshow(np.transpose(make_grid(x_hat.detach().cpu(), padding=2, normalize=True), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_random_sample_image(codebook, decoder, indices_shape=(batch_size//2, img_size[0]//4, img_size[1]//4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
